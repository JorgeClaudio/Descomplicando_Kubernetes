apiVersion: v1
items:
- metadata:
    annotations:
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2024-11-04T19:34:07Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: kind-control-plane
      kubernetes.io/os: linux
      node-role.kubernetes.io/control-plane: ""
      node.kubernetes.io/exclude-from-external-load-balancers: ""
    name: kind-control-plane
    resourceVersion: "563"
    uid: 72c66743-5ad9-4024-a1ca-11d89edfb263
  spec:
    podCIDR: 10.244.0.0/24
    podCIDRs:
    - 10.244.0.0/24
    providerID: kind://docker/kind/kind-control-plane
    taints:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
  status:
    addresses:
    - address: 172.18.0.5
      type: InternalIP
    - address: kind-control-plane
      type: Hostname
    allocatable:
      cpu: "12"
      ephemeral-storage: 490048472Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 16238156Ki
      pods: "110"
    capacity:
      cpu: "12"
      ephemeral-storage: 490048472Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 16238156Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2024-11-04T19:34:28Z"
      lastTransitionTime: "2024-11-04T19:34:05Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2024-11-04T19:34:28Z"
      lastTransitionTime: "2024-11-04T19:34:05Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2024-11-04T19:34:28Z"
      lastTransitionTime: "2024-11-04T19:34:05Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2024-11-04T19:34:28Z"
      lastTransitionTime: "2024-11-04T19:34:28Z"
      message: kubelet is posting ready status
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - docker.io/library/import-2024-08-13@sha256:eb4760466bf0dea0ed0a6557780dfdfb8a26bfe22302a9c023c1a45fc315ee03
      - registry.k8s.io/kube-apiserver-amd64:v1.31.0
      - registry.k8s.io/kube-apiserver:v1.31.0
      sizeBytes: 95227889
    - names:
      - docker.io/library/import-2024-08-13@sha256:fb36548b322947288048097367669a4c44acb7bcd0475659bfd788924efdc043
      - registry.k8s.io/kube-proxy-amd64:v1.31.0
      - registry.k8s.io/kube-proxy:v1.31.0
      sizeBytes: 92725434
    - names:
      - docker.io/library/import-2024-08-13@sha256:881959dffed0c747196d62ab29e6d5fe1f750b709b9703db47565afa0b6acec0
      - registry.k8s.io/kube-controller-manager-amd64:v1.31.0
      - registry.k8s.io/kube-controller-manager:v1.31.0
      sizeBytes: 89432392
    - names:
      - docker.io/library/import-2024-08-13@sha256:a4448b721006ab075a7142efcd329383db5b80a19c930f7fbf58cf6e2df1eb1b
      - registry.k8s.io/kube-scheduler-amd64:v1.31.0
      - registry.k8s.io/kube-scheduler:v1.31.0
      sizeBytes: 68415304
    - names:
      - registry.k8s.io/etcd:3.5.15-0
      sizeBytes: 56909194
    - names:
      - docker.io/kindest/kindnetd:v20240813-c6f155d6
      sizeBytes: 36793393
    - names:
      - docker.io/kindest/local-path-provisioner:v20240813-c6f155d6
      sizeBytes: 19430244
    - names:
      - registry.k8s.io/coredns/coredns:v1.11.1
      sizeBytes: 18182961
    - names:
      - docker.io/kindest/local-path-helper:v20230510-486859a6
      sizeBytes: 3052318
    - names:
      - registry.k8s.io/pause:3.10
      sizeBytes: 320368
    nodeInfo:
      architecture: amd64
      bootID: 9e055514-e64f-45c2-b14c-bfa133f0dd0a
      containerRuntimeVersion: containerd://1.7.18
      kernelVersion: 6.8.0-48-generic
      kubeProxyVersion: ""
      kubeletVersion: v1.31.0
      machineID: eb7b2a1242e5490cb5c76a0cffaa3d30
      operatingSystem: linux
      osImage: Debian GNU/Linux 12 (bookworm)
      systemUUID: 96e45b9d-23b3-4632-b393-9cc3651bf5f7
- metadata:
    annotations:
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2024-11-04T19:34:22Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: kind-worker
      kubernetes.io/os: linux
    name: kind-worker
    resourceVersion: "616"
    uid: e645302f-6781-4867-9a73-70228c9929b0
  spec:
    podCIDR: 10.244.2.0/24
    podCIDRs:
    - 10.244.2.0/24
    providerID: kind://docker/kind/kind-worker
  status:
    addresses:
    - address: 172.18.0.2
      type: InternalIP
    - address: kind-worker
      type: Hostname
    allocatable:
      cpu: "12"
      ephemeral-storage: 490048472Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 16238156Ki
      pods: "110"
    capacity:
      cpu: "12"
      ephemeral-storage: 490048472Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 16238156Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2024-11-04T19:34:35Z"
      lastTransitionTime: "2024-11-04T19:34:22Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2024-11-04T19:34:35Z"
      lastTransitionTime: "2024-11-04T19:34:22Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2024-11-04T19:34:35Z"
      lastTransitionTime: "2024-11-04T19:34:22Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2024-11-04T19:34:35Z"
      lastTransitionTime: "2024-11-04T19:34:35Z"
      message: kubelet is posting ready status
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - docker.io/library/import-2024-08-13@sha256:eb4760466bf0dea0ed0a6557780dfdfb8a26bfe22302a9c023c1a45fc315ee03
      - registry.k8s.io/kube-apiserver-amd64:v1.31.0
      - registry.k8s.io/kube-apiserver:v1.31.0
      sizeBytes: 95227889
    - names:
      - docker.io/library/import-2024-08-13@sha256:fb36548b322947288048097367669a4c44acb7bcd0475659bfd788924efdc043
      - registry.k8s.io/kube-proxy-amd64:v1.31.0
      - registry.k8s.io/kube-proxy:v1.31.0
      sizeBytes: 92725434
    - names:
      - docker.io/library/import-2024-08-13@sha256:881959dffed0c747196d62ab29e6d5fe1f750b709b9703db47565afa0b6acec0
      - registry.k8s.io/kube-controller-manager-amd64:v1.31.0
      - registry.k8s.io/kube-controller-manager:v1.31.0
      sizeBytes: 89432392
    - names:
      - docker.io/library/import-2024-08-13@sha256:a4448b721006ab075a7142efcd329383db5b80a19c930f7fbf58cf6e2df1eb1b
      - registry.k8s.io/kube-scheduler-amd64:v1.31.0
      - registry.k8s.io/kube-scheduler:v1.31.0
      sizeBytes: 68415304
    - names:
      - registry.k8s.io/etcd:3.5.15-0
      sizeBytes: 56909194
    - names:
      - docker.io/kindest/kindnetd:v20240813-c6f155d6
      sizeBytes: 36793393
    - names:
      - docker.io/kindest/local-path-provisioner:v20240813-c6f155d6
      sizeBytes: 19430244
    - names:
      - registry.k8s.io/coredns/coredns:v1.11.1
      sizeBytes: 18182961
    - names:
      - docker.io/kindest/local-path-helper:v20230510-486859a6
      sizeBytes: 3052318
    - names:
      - registry.k8s.io/pause:3.10
      sizeBytes: 320368
    nodeInfo:
      architecture: amd64
      bootID: 9e055514-e64f-45c2-b14c-bfa133f0dd0a
      containerRuntimeVersion: containerd://1.7.18
      kernelVersion: 6.8.0-48-generic
      kubeProxyVersion: ""
      kubeletVersion: v1.31.0
      machineID: 172bc6d8514840baa948d884490a7292
      operatingSystem: linux
      osImage: Debian GNU/Linux 12 (bookworm)
      systemUUID: 32512ad6-c7bd-4d14-90d8-70b989880a20
- metadata:
    annotations:
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2024-11-04T19:34:22Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: kind-worker2
      kubernetes.io/os: linux
    name: kind-worker2
    resourceVersion: "613"
    uid: f9cbe64a-206c-453a-b165-d02cacca1105
  spec:
    podCIDR: 10.244.1.0/24
    podCIDRs:
    - 10.244.1.0/24
    providerID: kind://docker/kind/kind-worker2
  status:
    addresses:
    - address: 172.18.0.3
      type: InternalIP
    - address: kind-worker2
      type: Hostname
    allocatable:
      cpu: "12"
      ephemeral-storage: 490048472Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 16238156Ki
      pods: "110"
    capacity:
      cpu: "12"
      ephemeral-storage: 490048472Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 16238156Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2024-11-04T19:34:34Z"
      lastTransitionTime: "2024-11-04T19:34:22Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2024-11-04T19:34:34Z"
      lastTransitionTime: "2024-11-04T19:34:22Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2024-11-04T19:34:34Z"
      lastTransitionTime: "2024-11-04T19:34:22Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2024-11-04T19:34:34Z"
      lastTransitionTime: "2024-11-04T19:34:34Z"
      message: kubelet is posting ready status
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - docker.io/library/import-2024-08-13@sha256:eb4760466bf0dea0ed0a6557780dfdfb8a26bfe22302a9c023c1a45fc315ee03
      - registry.k8s.io/kube-apiserver-amd64:v1.31.0
      - registry.k8s.io/kube-apiserver:v1.31.0
      sizeBytes: 95227889
    - names:
      - docker.io/library/import-2024-08-13@sha256:fb36548b322947288048097367669a4c44acb7bcd0475659bfd788924efdc043
      - registry.k8s.io/kube-proxy-amd64:v1.31.0
      - registry.k8s.io/kube-proxy:v1.31.0
      sizeBytes: 92725434
    - names:
      - docker.io/library/import-2024-08-13@sha256:881959dffed0c747196d62ab29e6d5fe1f750b709b9703db47565afa0b6acec0
      - registry.k8s.io/kube-controller-manager-amd64:v1.31.0
      - registry.k8s.io/kube-controller-manager:v1.31.0
      sizeBytes: 89432392
    - names:
      - docker.io/library/import-2024-08-13@sha256:a4448b721006ab075a7142efcd329383db5b80a19c930f7fbf58cf6e2df1eb1b
      - registry.k8s.io/kube-scheduler-amd64:v1.31.0
      - registry.k8s.io/kube-scheduler:v1.31.0
      sizeBytes: 68415304
    - names:
      - registry.k8s.io/etcd:3.5.15-0
      sizeBytes: 56909194
    - names:
      - docker.io/kindest/kindnetd:v20240813-c6f155d6
      sizeBytes: 36793393
    - names:
      - docker.io/kindest/local-path-provisioner:v20240813-c6f155d6
      sizeBytes: 19430244
    - names:
      - registry.k8s.io/coredns/coredns:v1.11.1
      sizeBytes: 18182961
    - names:
      - docker.io/kindest/local-path-helper:v20230510-486859a6
      sizeBytes: 3052318
    - names:
      - registry.k8s.io/pause:3.10
      sizeBytes: 320368
    nodeInfo:
      architecture: amd64
      bootID: 9e055514-e64f-45c2-b14c-bfa133f0dd0a
      containerRuntimeVersion: containerd://1.7.18
      kernelVersion: 6.8.0-48-generic
      kubeProxyVersion: ""
      kubeletVersion: v1.31.0
      machineID: 796e4600268941b88b9d543fe27d13b1
      operatingSystem: linux
      osImage: Debian GNU/Linux 12 (bookworm)
      systemUUID: 7cf40b2d-9885-427f-93ed-5dc0fef60342
- metadata:
    annotations:
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2024-11-04T19:34:22Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: kind-worker3
      kubernetes.io/os: linux
    name: kind-worker3
    resourceVersion: "619"
    uid: eb42267d-a6f3-400a-bd3b-7eecc9b01bcd
  spec:
    podCIDR: 10.244.3.0/24
    podCIDRs:
    - 10.244.3.0/24
    providerID: kind://docker/kind/kind-worker3
  status:
    addresses:
    - address: 172.18.0.4
      type: InternalIP
    - address: kind-worker3
      type: Hostname
    allocatable:
      cpu: "12"
      ephemeral-storage: 490048472Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 16238156Ki
      pods: "110"
    capacity:
      cpu: "12"
      ephemeral-storage: 490048472Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 16238156Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2024-11-04T19:34:35Z"
      lastTransitionTime: "2024-11-04T19:34:22Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2024-11-04T19:34:35Z"
      lastTransitionTime: "2024-11-04T19:34:22Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2024-11-04T19:34:35Z"
      lastTransitionTime: "2024-11-04T19:34:22Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2024-11-04T19:34:35Z"
      lastTransitionTime: "2024-11-04T19:34:35Z"
      message: kubelet is posting ready status
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - docker.io/library/import-2024-08-13@sha256:eb4760466bf0dea0ed0a6557780dfdfb8a26bfe22302a9c023c1a45fc315ee03
      - registry.k8s.io/kube-apiserver-amd64:v1.31.0
      - registry.k8s.io/kube-apiserver:v1.31.0
      sizeBytes: 95227889
    - names:
      - docker.io/library/import-2024-08-13@sha256:fb36548b322947288048097367669a4c44acb7bcd0475659bfd788924efdc043
      - registry.k8s.io/kube-proxy-amd64:v1.31.0
      - registry.k8s.io/kube-proxy:v1.31.0
      sizeBytes: 92725434
    - names:
      - docker.io/library/import-2024-08-13@sha256:881959dffed0c747196d62ab29e6d5fe1f750b709b9703db47565afa0b6acec0
      - registry.k8s.io/kube-controller-manager-amd64:v1.31.0
      - registry.k8s.io/kube-controller-manager:v1.31.0
      sizeBytes: 89432392
    - names:
      - docker.io/library/import-2024-08-13@sha256:a4448b721006ab075a7142efcd329383db5b80a19c930f7fbf58cf6e2df1eb1b
      - registry.k8s.io/kube-scheduler-amd64:v1.31.0
      - registry.k8s.io/kube-scheduler:v1.31.0
      sizeBytes: 68415304
    - names:
      - registry.k8s.io/etcd:3.5.15-0
      sizeBytes: 56909194
    - names:
      - docker.io/kindest/kindnetd:v20240813-c6f155d6
      sizeBytes: 36793393
    - names:
      - docker.io/kindest/local-path-provisioner:v20240813-c6f155d6
      sizeBytes: 19430244
    - names:
      - registry.k8s.io/coredns/coredns:v1.11.1
      sizeBytes: 18182961
    - names:
      - docker.io/kindest/local-path-helper:v20230510-486859a6
      sizeBytes: 3052318
    - names:
      - registry.k8s.io/pause:3.10
      sizeBytes: 320368
    nodeInfo:
      architecture: amd64
      bootID: 9e055514-e64f-45c2-b14c-bfa133f0dd0a
      containerRuntimeVersion: containerd://1.7.18
      kernelVersion: 6.8.0-48-generic
      kubeProxyVersion: ""
      kubeletVersion: v1.31.0
      machineID: 3cb5a708489f4281a081616d5068689b
      operatingSystem: linux
      osImage: Debian GNU/Linux 12 (bookworm)
      systemUUID: e58a008b-7bc6-40dd-91f7-8c7b07c2f7b1
kind: NodeList
metadata:
  resourceVersion: "848"
---
apiVersion: v1
items:
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:17Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-6f6b679f8f-c6l8s
    namespace: kube-system
    resourceVersion: "390"
    uid: f958a4b8-0373-4707-a4b2-c03448c6c955
  lastTimestamp: "2024-11-04T19:34:17Z"
  message: '0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready:
    }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.'
  metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    name: coredns-6f6b679f8f-c6l8s.1804daea61a7b274
    namespace: kube-system
    resourceVersion: "397"
    uid: e2df8890-3e19-4881-9128-39edadb81aa9
  reason: FailedScheduling
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:28Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-6f6b679f8f-c6l8s
    namespace: kube-system
    resourceVersion: "399"
    uid: f958a4b8-0373-4707-a4b2-c03448c6c955
  lastTimestamp: "2024-11-04T19:34:28Z"
  message: Successfully assigned kube-system/coredns-6f6b679f8f-c6l8s to kind-control-plane
  metadata:
    creationTimestamp: "2024-11-04T19:34:28Z"
    name: coredns-6f6b679f8f-c6l8s.1804daed1ba2779c
    namespace: kube-system
    resourceVersion: "568"
    uid: 7c2916a7-1a40-401d-844d-62c83ed6e08d
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:29Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-6f6b679f8f-c6l8s
    namespace: kube-system
    resourceVersion: "565"
    uid: f958a4b8-0373-4707-a4b2-c03448c6c955
  lastTimestamp: "2024-11-04T19:34:29Z"
  message: Container image "registry.k8s.io/coredns/coredns:v1.11.1" already present
    on machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:29Z"
    name: coredns-6f6b679f8f-c6l8s.1804daed350682bd
    namespace: kube-system
    resourceVersion: "575"
    uid: 48656b34-5982-45ea-9704-15cb7464ec28
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:29Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-6f6b679f8f-c6l8s
    namespace: kube-system
    resourceVersion: "565"
    uid: f958a4b8-0373-4707-a4b2-c03448c6c955
  lastTimestamp: "2024-11-04T19:34:29Z"
  message: Created container coredns
  metadata:
    creationTimestamp: "2024-11-04T19:34:29Z"
    name: coredns-6f6b679f8f-c6l8s.1804daed60e27a9e
    namespace: kube-system
    resourceVersion: "580"
    uid: 22adb148-435c-44b3-886c-b3d103da497e
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:29Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-6f6b679f8f-c6l8s
    namespace: kube-system
    resourceVersion: "565"
    uid: f958a4b8-0373-4707-a4b2-c03448c6c955
  lastTimestamp: "2024-11-04T19:34:29Z"
  message: Started container coredns
  metadata:
    creationTimestamp: "2024-11-04T19:34:29Z"
    name: coredns-6f6b679f8f-c6l8s.1804daed65d57307
    namespace: kube-system
    resourceVersion: "582"
    uid: 8636d6cd-0c4a-4dd3-b87f-0d6a33febafb
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:17Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-6f6b679f8f-zrxlc
    namespace: kube-system
    resourceVersion: "386"
    uid: 2c6b666f-716b-4bce-8530-a9619d2b02ef
  lastTimestamp: "2024-11-04T19:34:17Z"
  message: '0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready:
    }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.'
  metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    name: coredns-6f6b679f8f-zrxlc.1804daea61428c71
    namespace: kube-system
    resourceVersion: "389"
    uid: 32ef2075-2f55-4c93-924d-df65144183d4
  reason: FailedScheduling
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Warning
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:28Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: coredns-6f6b679f8f-zrxlc
    namespace: kube-system
    resourceVersion: "394"
    uid: 2c6b666f-716b-4bce-8530-a9619d2b02ef
  lastTimestamp: "2024-11-04T19:34:28Z"
  message: Successfully assigned kube-system/coredns-6f6b679f8f-zrxlc to kind-control-plane
  metadata:
    creationTimestamp: "2024-11-04T19:34:28Z"
    name: coredns-6f6b679f8f-zrxlc.1804daed1b8f99a7
    namespace: kube-system
    resourceVersion: "567"
    uid: b66fa537-d65e-4129-85de-c9fb5184f655
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:29Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-6f6b679f8f-zrxlc
    namespace: kube-system
    resourceVersion: "564"
    uid: 2c6b666f-716b-4bce-8530-a9619d2b02ef
  lastTimestamp: "2024-11-04T19:34:29Z"
  message: Container image "registry.k8s.io/coredns/coredns:v1.11.1" already present
    on machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:29Z"
    name: coredns-6f6b679f8f-zrxlc.1804daed34c2616c
    namespace: kube-system
    resourceVersion: "573"
    uid: 1a946a10-f905-4504-9e74-59a65048adb7
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:29Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-6f6b679f8f-zrxlc
    namespace: kube-system
    resourceVersion: "564"
    uid: 2c6b666f-716b-4bce-8530-a9619d2b02ef
  lastTimestamp: "2024-11-04T19:34:29Z"
  message: Created container coredns
  metadata:
    creationTimestamp: "2024-11-04T19:34:29Z"
    name: coredns-6f6b679f8f-zrxlc.1804daed60af01b7
    namespace: kube-system
    resourceVersion: "579"
    uid: 7753c4c0-9dc1-4071-810f-7db1cc5890d6
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:29Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{coredns}
    kind: Pod
    name: coredns-6f6b679f8f-zrxlc
    namespace: kube-system
    resourceVersion: "564"
    uid: 2c6b666f-716b-4bce-8530-a9619d2b02ef
  lastTimestamp: "2024-11-04T19:34:29Z"
  message: Started container coredns
  metadata:
    creationTimestamp: "2024-11-04T19:34:29Z"
    name: coredns-6f6b679f8f-zrxlc.1804daed65d58466
    namespace: kube-system
    resourceVersion: "583"
    uid: 91d9dfaf-d59c-4130-9411-b22ae2d20a17
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: ReplicaSet
    name: coredns-6f6b679f8f
    namespace: kube-system
    resourceVersion: "351"
    uid: 5046aef9-cd08-4232-94ec-fe108046f5e7
  lastTimestamp: "2024-11-04T19:34:17Z"
  message: 'Created pod: coredns-6f6b679f8f-zrxlc'
  metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    name: coredns-6f6b679f8f.1804daea613f372f
    namespace: kube-system
    resourceVersion: "388"
    uid: c4e38bce-40db-441e-a782-5b067286e4ca
  reason: SuccessfulCreate
  reportingComponent: replicaset-controller
  reportingInstance: ""
  source:
    component: replicaset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:17Z"
  involvedObject:
    apiVersion: apps/v1
    kind: ReplicaSet
    name: coredns-6f6b679f8f
    namespace: kube-system
    resourceVersion: "351"
    uid: 5046aef9-cd08-4232-94ec-fe108046f5e7
  lastTimestamp: "2024-11-04T19:34:17Z"
  message: 'Created pod: coredns-6f6b679f8f-c6l8s'
  metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    name: coredns-6f6b679f8f.1804daea616ea844
    namespace: kube-system
    resourceVersion: "393"
    uid: 6785676c-1ca9-417e-9d97-2fe87aa7c7e1
  reason: SuccessfulCreate
  reportingComponent: replicaset-controller
  reportingInstance: ""
  source:
    component: replicaset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:16Z"
  involvedObject:
    apiVersion: apps/v1
    kind: Deployment
    name: coredns
    namespace: kube-system
    resourceVersion: "267"
    uid: 59129153-fea2-4c26-b849-3a3c2a5ff52b
  lastTimestamp: "2024-11-04T19:34:16Z"
  message: Scaled up replica set coredns-6f6b679f8f to 2
  metadata:
    creationTimestamp: "2024-11-04T19:34:16Z"
    name: coredns.1804daea3dde5f9e
    namespace: kube-system
    resourceVersion: "354"
    uid: 7c08c8ab-b52e-488b-9c69-633eb91f2967
  reason: ScalingReplicaSet
  reportingComponent: deployment-controller
  reportingInstance: ""
  source:
    component: deployment-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:04Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{etcd}
    kind: Pod
    name: etcd-kind-control-plane
    namespace: kube-system
    uid: f018de87cf623341cfa9d9ff09862466
  lastTimestamp: "2024-11-04T19:34:04Z"
  message: Container image "registry.k8s.io/etcd:3.5.15-0" already present on machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: etcd-kind-control-plane.1804dae775564d68
    namespace: kube-system
    resourceVersion: "93"
    uid: 1a33c03b-ce61-47a5-9d40-3d1b4cffb832
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:05Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{etcd}
    kind: Pod
    name: etcd-kind-control-plane
    namespace: kube-system
    uid: f018de87cf623341cfa9d9ff09862466
  lastTimestamp: "2024-11-04T19:34:05Z"
  message: Created container etcd
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: etcd-kind-control-plane.1804dae7aefe5bac
    namespace: kube-system
    resourceVersion: "101"
    uid: 4d2899e1-4837-4fcf-a75e-a5bac5753980
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:05Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{etcd}
    kind: Pod
    name: etcd-kind-control-plane
    namespace: kube-system
    uid: f018de87cf623341cfa9d9ff09862466
  lastTimestamp: "2024-11-04T19:34:05Z"
  message: Started container etcd
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: etcd-kind-control-plane.1804dae7b3caf10d
    namespace: kube-system
    resourceVersion: "102"
    uid: afc55b3e-7252-4a18-9299-ad0668fce39d
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kindnet-6f8tr
    namespace: kube-system
    resourceVersion: "493"
    uid: a644f054-ad1e-467c-9549-a7f39b0c8201
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Successfully assigned kube-system/kindnet-6f8tr to kind-worker3
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kindnet-6f8tr.1804daebab69982a
    namespace: kube-system
    resourceVersion: "503"
    uid: f70e090c-3017-4d15-9727-7a3ed9a728c6
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:23Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-6f8tr
    namespace: kube-system
    resourceVersion: "499"
    uid: a644f054-ad1e-467c-9549-a7f39b0c8201
  lastTimestamp: "2024-11-04T19:34:23Z"
  message: Container image "docker.io/kindest/kindnetd:v20240813-c6f155d6" already
    present on machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:23Z"
    name: kindnet-6f8tr.1804daec0169b7ef
    namespace: kube-system
    resourceVersion: "524"
    uid: 14cc215d-548f-4212-bddf-ba1b1e09fca0
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-6f8tr
    namespace: kube-system
    resourceVersion: "499"
    uid: a644f054-ad1e-467c-9549-a7f39b0c8201
  lastTimestamp: "2024-11-04T19:34:24Z"
  message: Created container kindnet-cni
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kindnet-6f8tr.1804daec22e808e1
    namespace: kube-system
    resourceVersion: "542"
    uid: cb115bce-9b3f-4116-a740-2673dfda1f0f
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-6f8tr
    namespace: kube-system
    resourceVersion: "499"
    uid: a644f054-ad1e-467c-9549-a7f39b0c8201
  lastTimestamp: "2024-11-04T19:34:24Z"
  message: Started container kindnet-cni
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kindnet-6f8tr.1804daec3510410c
    namespace: kube-system
    resourceVersion: "545"
    uid: dcdea74d-d589-4a2c-b971-fb713b34e843
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kindnet-c2xls
    namespace: kube-system
    resourceVersion: "475"
    uid: 66e3d178-7fe4-4838-8bf4-ed872312b671
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Successfully assigned kube-system/kindnet-c2xls to kind-worker
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kindnet-c2xls.1804daebaac20f6b
    namespace: kube-system
    resourceVersion: "492"
    uid: 3eeba2c6-4218-449c-a87e-c78a9ee81ba3
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:23Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-c2xls
    namespace: kube-system
    resourceVersion: "480"
    uid: 66e3d178-7fe4-4838-8bf4-ed872312b671
  lastTimestamp: "2024-11-04T19:34:23Z"
  message: Container image "docker.io/kindest/kindnetd:v20240813-c6f155d6" already
    present on machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:23Z"
    name: kindnet-c2xls.1804daec0159f800
    namespace: kube-system
    resourceVersion: "523"
    uid: bc26e004-03cd-428d-8bf3-c9803057bb05
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-c2xls
    namespace: kube-system
    resourceVersion: "480"
    uid: 66e3d178-7fe4-4838-8bf4-ed872312b671
  lastTimestamp: "2024-11-04T19:34:24Z"
  message: Created container kindnet-cni
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kindnet-c2xls.1804daec2223d90d
    namespace: kube-system
    resourceVersion: "541"
    uid: 870bc6bd-ec47-4271-90a5-b7e8c6e7ef52
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-c2xls
    namespace: kube-system
    resourceVersion: "480"
    uid: 66e3d178-7fe4-4838-8bf4-ed872312b671
  lastTimestamp: "2024-11-04T19:34:24Z"
  message: Started container kindnet-cni
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kindnet-c2xls.1804daec311b2e80
    namespace: kube-system
    resourceVersion: "544"
    uid: d2ee532a-cdfb-4c2f-a456-98b74240679a
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:16Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kindnet-sl57q
    namespace: kube-system
    resourceVersion: "375"
    uid: 7e45cff7-2f1a-437b-9589-90e29fc1ae75
  lastTimestamp: "2024-11-04T19:34:16Z"
  message: Successfully assigned kube-system/kindnet-sl57q to kind-control-plane
  metadata:
    creationTimestamp: "2024-11-04T19:34:16Z"
    name: kindnet-sl57q.1804daea5ead8f6b
    namespace: kube-system
    resourceVersion: "383"
    uid: e025b5c2-a8fa-46be-81e3-60bc1b4a649d
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:17Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-sl57q
    namespace: kube-system
    resourceVersion: "379"
    uid: 7e45cff7-2f1a-437b-9589-90e29fc1ae75
  lastTimestamp: "2024-11-04T19:34:17Z"
  message: Container image "docker.io/kindest/kindnetd:v20240813-c6f155d6" already
    present on machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    name: kindnet-sl57q.1804daea7e433c30
    namespace: kube-system
    resourceVersion: "408"
    uid: a9c30220-4085-4482-9143-fb3859526434
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:17Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-sl57q
    namespace: kube-system
    resourceVersion: "379"
    uid: 7e45cff7-2f1a-437b-9589-90e29fc1ae75
  lastTimestamp: "2024-11-04T19:34:17Z"
  message: Created container kindnet-cni
  metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    name: kindnet-sl57q.1804daea9a1eb6db
    namespace: kube-system
    resourceVersion: "414"
    uid: 663b61ae-b8b5-48a0-b474-ecd1bb931884
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:18Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-sl57q
    namespace: kube-system
    resourceVersion: "379"
    uid: 7e45cff7-2f1a-437b-9589-90e29fc1ae75
  lastTimestamp: "2024-11-04T19:34:18Z"
  message: Started container kindnet-cni
  metadata:
    creationTimestamp: "2024-11-04T19:34:18Z"
    name: kindnet-sl57q.1804daeaae0ce18a
    namespace: kube-system
    resourceVersion: "416"
    uid: d6af5728-7fe7-4328-9a25-8ffe98b26888
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kindnet-wrm8q
    namespace: kube-system
    resourceVersion: "448"
    uid: 4b551755-e7d7-4c00-aa94-3ccee75e3672
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Successfully assigned kube-system/kindnet-wrm8q to kind-worker2
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kindnet-wrm8q.1804daeb9c5969ca
    namespace: kube-system
    resourceVersion: "458"
    uid: 90882035-7dde-4005-8918-6b921141aa81
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:23Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-wrm8q
    namespace: kube-system
    resourceVersion: "453"
    uid: 4b551755-e7d7-4c00-aa94-3ccee75e3672
  lastTimestamp: "2024-11-04T19:34:23Z"
  message: Container image "docker.io/kindest/kindnetd:v20240813-c6f155d6" already
    present on machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:23Z"
    name: kindnet-wrm8q.1804daebef1e5b18
    namespace: kube-system
    resourceVersion: "519"
    uid: e169f24f-723a-4a02-bd9a-d8efdaf9ac1c
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-wrm8q
    namespace: kube-system
    resourceVersion: "453"
    uid: 4b551755-e7d7-4c00-aa94-3ccee75e3672
  lastTimestamp: "2024-11-04T19:34:24Z"
  message: Created container kindnet-cni
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kindnet-wrm8q.1804daec105f1f5a
    namespace: kube-system
    resourceVersion: "530"
    uid: ccc4a788-c35d-4471-8ca0-c2e3e557bcbc
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kindnet-cni}
    kind: Pod
    name: kindnet-wrm8q
    namespace: kube-system
    resourceVersion: "453"
    uid: 4b551755-e7d7-4c00-aa94-3ccee75e3672
  lastTimestamp: "2024-11-04T19:34:24Z"
  message: Started container kindnet-cni
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kindnet-wrm8q.1804daec2694c125
    namespace: kube-system
    resourceVersion: "543"
    uid: 0b1fc6ff-3b9b-4db3-850a-631884844c27
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:16Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: kindnet
    namespace: kube-system
    resourceVersion: "286"
    uid: f3a8761c-bae0-41a2-8d67-e534e8e76570
  lastTimestamp: "2024-11-04T19:34:16Z"
  message: 'Created pod: kindnet-sl57q'
  metadata:
    creationTimestamp: "2024-11-04T19:34:16Z"
    name: kindnet.1804daea5e7c5e55
    namespace: kube-system
    resourceVersion: "381"
    uid: 587c97c2-382b-495e-9c47-dafc8a7d2697
  reason: SuccessfulCreate
  reportingComponent: daemonset-controller
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: kindnet
    namespace: kube-system
    resourceVersion: "418"
    uid: f3a8761c-bae0-41a2-8d67-e534e8e76570
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Created pod: kindnet-wrm8q'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kindnet.1804daeb9c2621ad
    namespace: kube-system
    resourceVersion: "459"
    uid: d7c644c0-f8b5-4be7-b816-284b4b6c224c
  reason: SuccessfulCreate
  reportingComponent: daemonset-controller
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: kindnet
    namespace: kube-system
    resourceVersion: "456"
    uid: f3a8761c-bae0-41a2-8d67-e534e8e76570
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Created pod: kindnet-c2xls'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kindnet.1804daebaa8ec286
    namespace: kube-system
    resourceVersion: "479"
    uid: 9998a9a1-bb41-4236-b6e1-04ab048af044
  reason: SuccessfulCreate
  reportingComponent: daemonset-controller
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: kindnet
    namespace: kube-system
    resourceVersion: "483"
    uid: f3a8761c-bae0-41a2-8d67-e534e8e76570
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Created pod: kindnet-6f8tr'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kindnet.1804daebab3c8ad0
    namespace: kube-system
    resourceVersion: "501"
    uid: 77815302-9b0d-4969-a15b-2025c9cc60a6
  reason: SuccessfulCreate
  reportingComponent: daemonset-controller
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:04Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-apiserver}
    kind: Pod
    name: kube-apiserver-kind-control-plane
    namespace: kube-system
    uid: daf3d0450842a74f77f22ba9e10a23d9
  lastTimestamp: "2024-11-04T19:34:04Z"
  message: Container image "registry.k8s.io/kube-apiserver:v1.31.0" already present
    on machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kube-apiserver-kind-control-plane.1804dae775554052
    namespace: kube-system
    resourceVersion: "92"
    uid: e53f902c-a328-4c92-8455-1710b168081c
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:05Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-apiserver}
    kind: Pod
    name: kube-apiserver-kind-control-plane
    namespace: kube-system
    uid: daf3d0450842a74f77f22ba9e10a23d9
  lastTimestamp: "2024-11-04T19:34:05Z"
  message: Created container kube-apiserver
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kube-apiserver-kind-control-plane.1804dae7999acb83
    namespace: kube-system
    resourceVersion: "97"
    uid: da79ed30-c4b9-4d0b-8a18-e776ab7eba09
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:05Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-apiserver}
    kind: Pod
    name: kube-apiserver-kind-control-plane
    namespace: kube-system
    uid: daf3d0450842a74f77f22ba9e10a23d9
  lastTimestamp: "2024-11-04T19:34:05Z"
  message: Started container kube-apiserver
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kube-apiserver-kind-control-plane.1804dae79e6779e5
    namespace: kube-system
    resourceVersion: "99"
    uid: 20eba397-90b9-4ca2-b5ef-3a4927b12bc9
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:04Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-controller-manager}
    kind: Pod
    name: kube-controller-manager-kind-control-plane
    namespace: kube-system
    uid: 7e9e9ec5e3483420ce6070ae7308f53f
  lastTimestamp: "2024-11-04T19:34:04Z"
  message: Container image "registry.k8s.io/kube-controller-manager:v1.31.0" already
    present on machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kube-controller-manager-kind-control-plane.1804dae775682414
    namespace: kube-system
    resourceVersion: "94"
    uid: 889c4e63-fcbc-460d-aabe-d496d6c942ca
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:05Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-controller-manager}
    kind: Pod
    name: kube-controller-manager-kind-control-plane
    namespace: kube-system
    uid: 7e9e9ec5e3483420ce6070ae7308f53f
  lastTimestamp: "2024-11-04T19:34:05Z"
  message: Created container kube-controller-manager
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kube-controller-manager-kind-control-plane.1804dae79bf26ead
    namespace: kube-system
    resourceVersion: "98"
    uid: af4dff58-49e4-4e57-984d-c8c3a51104a5
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:05Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-controller-manager}
    kind: Pod
    name: kube-controller-manager-kind-control-plane
    namespace: kube-system
    uid: 7e9e9ec5e3483420ce6070ae7308f53f
  lastTimestamp: "2024-11-04T19:34:05Z"
  message: Started container kube-controller-manager
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kube-controller-manager-kind-control-plane.1804dae7a0b882a0
    namespace: kube-system
    resourceVersion: "100"
    uid: fd5d4b95-7af2-44c9-a027-0b76c52905c4
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:11Z"
  involvedObject:
    apiVersion: coordination.k8s.io/v1
    kind: Lease
    name: kube-controller-manager
    namespace: kube-system
    resourceVersion: "294"
    uid: 8c60a4ef-0816-4cee-b5db-a3f72229360b
  lastTimestamp: "2024-11-04T19:34:11Z"
  message: kind-control-plane_e59d2775-56b2-443c-b4cc-aeb10f0ca882 became leader
  metadata:
    creationTimestamp: "2024-11-04T19:34:11Z"
    name: kube-controller-manager.1804dae90730782a
    namespace: kube-system
    resourceVersion: "296"
    uid: cbde276a-216f-407f-91f7-ef7857efb02a
  reason: LeaderElection
  reportingComponent: kube-controller-manager
  reportingInstance: ""
  source:
    component: kube-controller-manager
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:16Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kube-proxy-88txq
    namespace: kube-system
    resourceVersion: "374"
    uid: 19a2b5f9-9032-4f13-8f2e-93cd901480e8
  lastTimestamp: "2024-11-04T19:34:16Z"
  message: Successfully assigned kube-system/kube-proxy-88txq to kind-control-plane
  metadata:
    creationTimestamp: "2024-11-04T19:34:16Z"
    name: kube-proxy-88txq.1804daea5e94446b
    namespace: kube-system
    resourceVersion: "382"
    uid: 7c878179-c2db-4b2c-9a42-df1606c56a42
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:17Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-88txq
    namespace: kube-system
    resourceVersion: "376"
    uid: 19a2b5f9-9032-4f13-8f2e-93cd901480e8
  lastTimestamp: "2024-11-04T19:34:17Z"
  message: Container image "registry.k8s.io/kube-proxy:v1.31.0" already present on
    machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    name: kube-proxy-88txq.1804daea74e3a822
    namespace: kube-system
    resourceVersion: "407"
    uid: cc8a6d24-2249-4834-9703-b30c3289f813
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:17Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-88txq
    namespace: kube-system
    resourceVersion: "376"
    uid: 19a2b5f9-9032-4f13-8f2e-93cd901480e8
  lastTimestamp: "2024-11-04T19:34:17Z"
  message: Created container kube-proxy
  metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    name: kube-proxy-88txq.1804daea8c00f6ad
    namespace: kube-system
    resourceVersion: "409"
    uid: ef357c0d-061d-44d8-9d2d-d0a0bb574c9f
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:17Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-88txq
    namespace: kube-system
    resourceVersion: "376"
    uid: 19a2b5f9-9032-4f13-8f2e-93cd901480e8
  lastTimestamp: "2024-11-04T19:34:17Z"
  message: Started container kube-proxy
  metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    name: kube-proxy-88txq.1804daea90ae08f0
    namespace: kube-system
    resourceVersion: "410"
    uid: 77ce847e-b916-4745-bbcb-36632625b2b8
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kube-proxy-hz92l
    namespace: kube-system
    resourceVersion: "449"
    uid: 0ad8fdf0-937b-491e-8d2e-b9f21f8e45ef
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Successfully assigned kube-system/kube-proxy-hz92l to kind-worker2
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kube-proxy-hz92l.1804daeb9c5ac8cf
    namespace: kube-system
    resourceVersion: "460"
    uid: 5392a7eb-1ec2-47b5-bdfb-88c6bf20d615
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:23Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-hz92l
    namespace: kube-system
    resourceVersion: "452"
    uid: 0ad8fdf0-937b-491e-8d2e-b9f21f8e45ef
  lastTimestamp: "2024-11-04T19:34:23Z"
  message: Container image "registry.k8s.io/kube-proxy:v1.31.0" already present on
    machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:23Z"
    name: kube-proxy-hz92l.1804daebe56533f8
    namespace: kube-system
    resourceVersion: "518"
    uid: 080aee6a-ee9f-448d-99e3-89331dba4fea
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:23Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-hz92l
    namespace: kube-system
    resourceVersion: "452"
    uid: 0ad8fdf0-937b-491e-8d2e-b9f21f8e45ef
  lastTimestamp: "2024-11-04T19:34:23Z"
  message: Created container kube-proxy
  metadata:
    creationTimestamp: "2024-11-04T19:34:23Z"
    name: kube-proxy-hz92l.1804daebfc90b91d
    namespace: kube-system
    resourceVersion: "522"
    uid: ac874a8a-d076-45cf-993f-fb3bf5d4bab5
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:23Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-hz92l
    namespace: kube-system
    resourceVersion: "452"
    uid: 0ad8fdf0-937b-491e-8d2e-b9f21f8e45ef
  lastTimestamp: "2024-11-04T19:34:23Z"
  message: Started container kube-proxy
  metadata:
    creationTimestamp: "2024-11-04T19:34:23Z"
    name: kube-proxy-hz92l.1804daec018c1d07
    namespace: kube-system
    resourceVersion: "525"
    uid: 39aa9e16-90d9-4886-b4df-18fd6672ea5d
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kube-proxy-k8vnb
    namespace: kube-system
    resourceVersion: "490"
    uid: 0920d0c3-7bbd-4bf1-abc7-0214f4ccc2c5
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Successfully assigned kube-system/kube-proxy-k8vnb to kind-worker3
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kube-proxy-k8vnb.1804daebab4cbbc7
    namespace: kube-system
    resourceVersion: "500"
    uid: 461a9965-952b-4a97-bc1e-f869a7991f46
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:23Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-k8vnb
    namespace: kube-system
    resourceVersion: "495"
    uid: 0920d0c3-7bbd-4bf1-abc7-0214f4ccc2c5
  lastTimestamp: "2024-11-04T19:34:23Z"
  message: Container image "registry.k8s.io/kube-proxy:v1.31.0" already present on
    machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:23Z"
    name: kube-proxy-k8vnb.1804daebf5fe4b67
    namespace: kube-system
    resourceVersion: "521"
    uid: 7f3c07eb-18b9-4268-a128-f2311ed47754
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-k8vnb
    namespace: kube-system
    resourceVersion: "495"
    uid: 0920d0c3-7bbd-4bf1-abc7-0214f4ccc2c5
  lastTimestamp: "2024-11-04T19:34:24Z"
  message: Created container kube-proxy
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kube-proxy-k8vnb.1804daec133b32aa
    namespace: kube-system
    resourceVersion: "532"
    uid: 98097ffb-e066-4cf4-9b42-fcd596624352
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-k8vnb
    namespace: kube-system
    resourceVersion: "495"
    uid: 0920d0c3-7bbd-4bf1-abc7-0214f4ccc2c5
  lastTimestamp: "2024-11-04T19:34:24Z"
  message: Started container kube-proxy
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kube-proxy-k8vnb.1804daec176bcbf8
    namespace: kube-system
    resourceVersion: "534"
    uid: d817e156-7f07-41ee-ac48-a5f0d8b17215
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: v1
    kind: Pod
    name: kube-proxy-ngkmh
    namespace: kube-system
    resourceVersion: "476"
    uid: 28a168a8-3726-40ad-a696-3bbbe0d70603
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Successfully assigned kube-system/kube-proxy-ngkmh to kind-worker
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kube-proxy-ngkmh.1804daebaac1ecdc
    namespace: kube-system
    resourceVersion: "487"
    uid: 8d8b6d2d-b3b0-4ad8-af35-6388b7843b5b
  reason: Scheduled
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:23Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-ngkmh
    namespace: kube-system
    resourceVersion: "481"
    uid: 28a168a8-3726-40ad-a696-3bbbe0d70603
  lastTimestamp: "2024-11-04T19:34:23Z"
  message: Container image "registry.k8s.io/kube-proxy:v1.31.0" already present on
    machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:23Z"
    name: kube-proxy-ngkmh.1804daebf4bfb956
    namespace: kube-system
    resourceVersion: "520"
    uid: 14857457-3e63-4078-861c-802121aebe0f
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-ngkmh
    namespace: kube-system
    resourceVersion: "481"
    uid: 28a168a8-3726-40ad-a696-3bbbe0d70603
  lastTimestamp: "2024-11-04T19:34:24Z"
  message: Created container kube-proxy
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kube-proxy-ngkmh.1804daec123415f7
    namespace: kube-system
    resourceVersion: "531"
    uid: e0ae0775-144b-494e-88bc-2693225ff786
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:24Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-proxy}
    kind: Pod
    name: kube-proxy-ngkmh
    namespace: kube-system
    resourceVersion: "481"
    uid: 28a168a8-3726-40ad-a696-3bbbe0d70603
  lastTimestamp: "2024-11-04T19:34:24Z"
  message: Started container kube-proxy
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kube-proxy-ngkmh.1804daec175117e3
    namespace: kube-system
    resourceVersion: "533"
    uid: beb4ad77-433c-40d5-a187-e15972fbb613
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:16Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "271"
    uid: b17c7366-93c3-42e3-90b8-8b68c201469a
  lastTimestamp: "2024-11-04T19:34:16Z"
  message: 'Created pod: kube-proxy-88txq'
  metadata:
    creationTimestamp: "2024-11-04T19:34:16Z"
    name: kube-proxy.1804daea5e6a7b74
    namespace: kube-system
    resourceVersion: "377"
    uid: 08878bfc-91e8-441c-8168-c7bc2c010d1a
  reason: SuccessfulCreate
  reportingComponent: daemonset-controller
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "412"
    uid: b17c7366-93c3-42e3-90b8-8b68c201469a
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Created pod: kube-proxy-hz92l'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kube-proxy.1804daeb9c261ab5
    namespace: kube-system
    resourceVersion: "454"
    uid: 4c1f3a57-5176-4fe6-becb-671edb7b386b
  reason: SuccessfulCreate
  reportingComponent: daemonset-controller
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "457"
    uid: b17c7366-93c3-42e3-90b8-8b68c201469a
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Created pod: kube-proxy-ngkmh'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kube-proxy.1804daebaa8fbeb2
    namespace: kube-system
    resourceVersion: "486"
    uid: dc954f5e-2554-43e8-8d2f-03c989ec7023
  reason: SuccessfulCreate
  reportingComponent: daemonset-controller
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    apiVersion: apps/v1
    kind: DaemonSet
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "482"
    uid: b17c7366-93c3-42e3-90b8-8b68c201469a
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Created pod: kube-proxy-k8vnb'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kube-proxy.1804daebab21777f
    namespace: kube-system
    resourceVersion: "494"
    uid: bf5a114e-cab9-4dc2-91b8-8551479d2f1f
  reason: SuccessfulCreate
  reportingComponent: daemonset-controller
  reportingInstance: ""
  source:
    component: daemonset-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:04Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-scheduler}
    kind: Pod
    name: kube-scheduler-kind-control-plane
    namespace: kube-system
    uid: 216366b07956e646a3d2a2698a971f74
  lastTimestamp: "2024-11-04T19:34:04Z"
  message: Container image "registry.k8s.io/kube-scheduler:v1.31.0" already present
    on machine
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kube-scheduler-kind-control-plane.1804dae77554effe
    namespace: kube-system
    resourceVersion: "91"
    uid: a3c5d5a2-7d68-4bd9-9203-958dbb080916
  reason: Pulled
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:04Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-scheduler}
    kind: Pod
    name: kube-scheduler-kind-control-plane
    namespace: kube-system
    uid: 216366b07956e646a3d2a2698a971f74
  lastTimestamp: "2024-11-04T19:34:04Z"
  message: Created container kube-scheduler
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kube-scheduler-kind-control-plane.1804dae79468aa62
    namespace: kube-system
    resourceVersion: "95"
    uid: 6f220732-6754-4add-b5d0-ce7c26e26a09
  reason: Created
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:05Z"
  involvedObject:
    apiVersion: v1
    fieldPath: spec.containers{kube-scheduler}
    kind: Pod
    name: kube-scheduler-kind-control-plane
    namespace: kube-system
    uid: 216366b07956e646a3d2a2698a971f74
  lastTimestamp: "2024-11-04T19:34:05Z"
  message: Started container kube-scheduler
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kube-scheduler-kind-control-plane.1804dae799715ca9
    namespace: kube-system
    resourceVersion: "96"
    uid: 012db9c1-7560-4d79-927f-aa8a3c13bb6e
  reason: Started
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:11Z"
  involvedObject:
    apiVersion: coordination.k8s.io/v1
    kind: Lease
    name: kube-scheduler
    namespace: kube-system
    resourceVersion: "304"
    uid: c011e409-bc0e-4966-8ec9-78f5865f0342
  lastTimestamp: "2024-11-04T19:34:11Z"
  message: kind-control-plane_a520ce11-2490-4ff5-8df5-e5c35f8c29a2 became leader
  metadata:
    creationTimestamp: "2024-11-04T19:34:11Z"
    name: kube-scheduler.1804dae9088ec82a
    namespace: kube-system
    resourceVersion: "305"
    uid: caf79ab2-26f3-4dd7-a4ab-1ab322cddb19
  reason: LeaderElection
  reportingComponent: default-scheduler
  reportingInstance: ""
  source:
    component: default-scheduler
  type: Normal
kind: EventList
metadata:
  resourceVersion: "848"
---
apiVersion: v1
items: []
kind: ReplicationControllerList
metadata:
  resourceVersion: "848"
---
apiVersion: v1
items:
- metadata:
    annotations:
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2024-11-04T19:34:09Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "269"
    uid: fdeee145-3b49-4285-a02a-fe8d38d91bc9
  spec:
    clusterIP: 10.96.0.10
    clusterIPs:
    - 10.96.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
kind: ServiceList
metadata:
  resourceVersion: "848"
---
apiVersion: apps/v1
items:
- metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-11-04T19:34:10Z"
    generation: 1
    labels:
      app: kindnet
      k8s-app: kindnet
      tier: node
    name: kindnet
    namespace: kube-system
    resourceVersion: "553"
    uid: f3a8761c-bae0-41a2-8d67-e534e8e76570
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kindnet
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: kindnet
          k8s-app: kindnet
          tier: node
      spec:
        containers:
        - env:
          - name: HOST_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIP
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_SUBNET
            value: 10.244.0.0/16
          - name: CONTROL_PLANE_ENDPOINT
            value: kind-control-plane:6443
          image: docker.io/kindest/kindnetd:v20240813-c6f155d6
          imagePullPolicy: IfNotPresent
          name: kindnet-cni
          resources:
            limits:
              cpu: 100m
              memory: 50Mi
            requests:
              cpu: 100m
              memory: 50Mi
          securityContext:
            capabilities:
              add:
              - NET_RAW
              - NET_ADMIN
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cni/net.d
            name: cni-cfg
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kindnet
        serviceAccountName: kindnet
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /etc/cni/net.d
            type: ""
          name: cni-cfg
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2024-11-04T19:34:09Z"
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "538"
    uid: b17c7366-93c3-42e3-90b8-8b68c201469a
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --config=/var/lib/kube-proxy/config.conf
          - --hostname-override=$(NODE_NAME)
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.k8s.io/kube-proxy:v1.31.0
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
kind: DaemonSetList
metadata:
  resourceVersion: "848"
---
apiVersion: apps/v1
items:
- metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-11-04T19:34:09Z"
    generation: 1
    labels:
      k8s-app: kube-dns
    name: coredns
    namespace: kube-system
    resourceVersion: "598"
    uid: 59129153-fea2-4c26-b849-3a3c2a5ff52b
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: registry.k8s.io/coredns/coredns:v1.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2024-11-04T19:34:30Z"
      lastUpdateTime: "2024-11-04T19:34:30Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2024-11-04T19:34:16Z"
      lastUpdateTime: "2024-11-04T19:34:30Z"
      message: ReplicaSet "coredns-6f6b679f8f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
kind: DeploymentList
metadata:
  resourceVersion: "848"
---
apiVersion: apps/v1
items:
- metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2024-11-04T19:34:16Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 6f6b679f8f
    name: coredns-6f6b679f8f
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: 59129153-fea2-4c26-b849-3a3c2a5ff52b
    resourceVersion: "596"
    uid: 5046aef9-cd08-4232-94ec-fe108046f5e7
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 6f6b679f8f
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 6f6b679f8f
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: registry.k8s.io/coredns/coredns:v1.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
kind: ReplicaSetList
metadata:
  resourceVersion: "848"
---
apiVersion: v1
items:
- metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    generateName: coredns-6f6b679f8f-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 6f6b679f8f
    name: coredns-6f6b679f8f-c6l8s
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-6f6b679f8f
      uid: 5046aef9-cd08-4232-94ec-fe108046f5e7
    resourceVersion: "588"
    uid: f958a4b8-0373-4707-a4b2-c03448c6c955
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bn9tf
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: kind-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-bn9tf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e3da0441ed782858df1566e14a9c06c8ed398b172384882517c7c6cd4251e886
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imageID: sha256:cbb01a7bd410dc08ba382018ab909a674fb0e48687f0c00797ed5bc34fcc6bb4
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:29Z"
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bn9tf
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.18.0.5
    hostIPs:
    - ip: 172.18.0.5
    phase: Running
    podIP: 10.244.0.4
    podIPs:
    - ip: 10.244.0.4
    qosClass: Burstable
    startTime: "2024-11-04T19:34:28Z"
- metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    generateName: coredns-6f6b679f8f-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 6f6b679f8f
    name: coredns-6f6b679f8f-zrxlc
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-6f6b679f8f
      uid: 5046aef9-cd08-4232-94ec-fe108046f5e7
    resourceVersion: "592"
    uid: 2c6b666f-716b-4bce-8530-a9619d2b02ef
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j2mmj
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: kind-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-j2mmj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:30Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6712ae2929af75540440ef559927190b28eb55bc5175fdc6a61e7d15b926385a
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imageID: sha256:cbb01a7bd410dc08ba382018ab909a674fb0e48687f0c00797ed5bc34fcc6bb4
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:29Z"
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j2mmj
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.18.0.5
    hostIPs:
    - ip: 172.18.0.5
    phase: Running
    podIP: 10.244.0.2
    podIPs:
    - ip: 10.244.0.2
    qosClass: Burstable
    startTime: "2024-11-04T19:34:28Z"
- metadata:
    annotations:
      kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.5:2379
      kubernetes.io/config.hash: f018de87cf623341cfa9d9ff09862466
      kubernetes.io/config.mirror: f018de87cf623341cfa9d9ff09862466
      kubernetes.io/config.seen: "2024-11-04T19:34:03.894827657Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-11-04T19:34:07Z"
    labels:
      component: etcd
      tier: control-plane
    name: etcd-kind-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kind-control-plane
      uid: 72c66743-5ad9-4024-a1ca-11d89edfb263
    resourceVersion: "361"
    uid: 3640547c-512d-4320-a58c-020f50a8b27f
  spec:
    containers:
    - command:
      - etcd
      - --advertise-client-urls=https://172.18.0.5:2379
      - --cert-file=/etc/kubernetes/pki/etcd/server.crt
      - --client-cert-auth=true
      - --data-dir=/var/lib/etcd
      - --experimental-initial-corrupt-check=true
      - --experimental-watch-progress-notify-interval=5s
      - --initial-advertise-peer-urls=https://172.18.0.5:2380
      - --initial-cluster=kind-control-plane=https://172.18.0.5:2380
      - --key-file=/etc/kubernetes/pki/etcd/server.key
      - --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.5:2379
      - --listen-metrics-urls=http://127.0.0.1:2381
      - --listen-peer-urls=https://172.18.0.5:2380
      - --name=kind-control-plane
      - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      - --peer-client-cert-auth=true
      - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      - --snapshot-count=10000
      - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      image: registry.k8s.io/etcd:3.5.15-0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /livez
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: etcd
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          path: /readyz
          port: 2381
          scheme: HTTP
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 15
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /readyz
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/etcd
        name: etcd-data
      - mountPath: /etc/kubernetes/pki/etcd
        name: etcd-certs
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/pki/etcd
        type: DirectoryOrCreate
      name: etcd-certs
    - hostPath:
        path: /var/lib/etcd
        type: DirectoryOrCreate
      name: etcd-data
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://26628736855c41ed334f1f95905c2c450a57713005357ce823c67c74e7f226f1
      image: registry.k8s.io/etcd:3.5.15-0
      imageID: sha256:2e96e5913fc06e3d26915af3d0f2ca5048cc4b6327e661e80da792cbf8d8d9d4
      lastState: {}
      name: etcd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:05Z"
    hostIP: 172.18.0.5
    hostIPs:
    - ip: 172.18.0.5
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: Burstable
    startTime: "2024-11-04T19:34:08Z"
- metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    generateName: kindnet-
    labels:
      app: kindnet
      controller-revision-hash: 6878cc97bd
      k8s-app: kindnet
      pod-template-generation: "1"
      tier: node
    name: kindnet-6f8tr
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kindnet
      uid: f3a8761c-bae0-41a2-8d67-e534e8e76570
    resourceVersion: "551"
    uid: a644f054-ad1e-467c-9549-a7f39b0c8201
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker3
    containers:
    - env:
      - name: HOST_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_SUBNET
        value: 10.244.0.0/16
      - name: CONTROL_PLANE_ENDPOINT
        value: kind-control-plane:6443
      image: docker.io/kindest/kindnetd:v20240813-c6f155d6
      imagePullPolicy: IfNotPresent
      name: kindnet-cni
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_RAW
          - NET_ADMIN
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-58f47
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-worker3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kindnet
    serviceAccountName: kindnet
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-58f47
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://80b16ec44ca70bd97df95f95a0b88f76d8ce343844c588ad3c82b2b1dcafac67
      image: docker.io/kindest/kindnetd:v20240813-c6f155d6
      imageID: sha256:12968670680f4561ef6818782391eb120d6e3365cf3f967aad58749f95381a4f
      lastState: {}
      name: kindnet-cni
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:24Z"
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-58f47
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.18.0.4
    hostIPs:
    - ip: 172.18.0.4
    phase: Running
    podIP: 172.18.0.4
    podIPs:
    - ip: 172.18.0.4
    qosClass: Guaranteed
    startTime: "2024-11-04T19:34:23Z"
- metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    generateName: kindnet-
    labels:
      app: kindnet
      controller-revision-hash: 6878cc97bd
      k8s-app: kindnet
      pod-template-generation: "1"
      tier: node
    name: kindnet-c2xls
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kindnet
      uid: f3a8761c-bae0-41a2-8d67-e534e8e76570
    resourceVersion: "550"
    uid: 66e3d178-7fe4-4838-8bf4-ed872312b671
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker
    containers:
    - env:
      - name: HOST_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_SUBNET
        value: 10.244.0.0/16
      - name: CONTROL_PLANE_ENDPOINT
        value: kind-control-plane:6443
      image: docker.io/kindest/kindnetd:v20240813-c6f155d6
      imagePullPolicy: IfNotPresent
      name: kindnet-cni
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_RAW
          - NET_ADMIN
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ps6kr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-worker
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kindnet
    serviceAccountName: kindnet
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-ps6kr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://eaf96cf1dac960d3259f42f6f97dea4572aa6c7b544d07e85d4f9e866139ff88
      image: docker.io/kindest/kindnetd:v20240813-c6f155d6
      imageID: sha256:12968670680f4561ef6818782391eb120d6e3365cf3f967aad58749f95381a4f
      lastState: {}
      name: kindnet-cni
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:24Z"
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ps6kr
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: Guaranteed
    startTime: "2024-11-04T19:34:23Z"
- metadata:
    creationTimestamp: "2024-11-04T19:34:16Z"
    generateName: kindnet-
    labels:
      app: kindnet
      controller-revision-hash: 6878cc97bd
      k8s-app: kindnet
      pod-template-generation: "1"
      tier: node
    name: kindnet-sl57q
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kindnet
      uid: f3a8761c-bae0-41a2-8d67-e534e8e76570
    resourceVersion: "417"
    uid: 7e45cff7-2f1a-437b-9589-90e29fc1ae75
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-control-plane
    containers:
    - env:
      - name: HOST_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_SUBNET
        value: 10.244.0.0/16
      - name: CONTROL_PLANE_ENDPOINT
        value: kind-control-plane:6443
      image: docker.io/kindest/kindnetd:v20240813-c6f155d6
      imagePullPolicy: IfNotPresent
      name: kindnet-cni
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_RAW
          - NET_ADMIN
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zctl8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kindnet
    serviceAccountName: kindnet
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-zctl8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:18Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://de8000501055c5cc529bc9db891958f82a3189872c63fe53d462ddf1de99c959
      image: docker.io/kindest/kindnetd:v20240813-c6f155d6
      imageID: sha256:12968670680f4561ef6818782391eb120d6e3365cf3f967aad58749f95381a4f
      lastState: {}
      name: kindnet-cni
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:18Z"
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zctl8
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.18.0.5
    hostIPs:
    - ip: 172.18.0.5
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: Guaranteed
    startTime: "2024-11-04T19:34:16Z"
- metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    generateName: kindnet-
    labels:
      app: kindnet
      controller-revision-hash: 6878cc97bd
      k8s-app: kindnet
      pod-template-generation: "1"
      tier: node
    name: kindnet-wrm8q
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kindnet
      uid: f3a8761c-bae0-41a2-8d67-e534e8e76570
    resourceVersion: "546"
    uid: 4b551755-e7d7-4c00-aa94-3ccee75e3672
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker2
    containers:
    - env:
      - name: HOST_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_SUBNET
        value: 10.244.0.0/16
      - name: CONTROL_PLANE_ENDPOINT
        value: kind-control-plane:6443
      image: docker.io/kindest/kindnetd:v20240813-c6f155d6
      imagePullPolicy: IfNotPresent
      name: kindnet-cni
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_RAW
          - NET_ADMIN
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9sz4l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-worker2
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kindnet
    serviceAccountName: kindnet
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-9sz4l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://07e84aac68fed124a59bfbf460a9b05aae20354afb8078064b1fa974825267d2
      image: docker.io/kindest/kindnetd:v20240813-c6f155d6
      imageID: sha256:12968670680f4561ef6818782391eb120d6e3365cf3f967aad58749f95381a4f
      lastState: {}
      name: kindnet-cni
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:24Z"
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9sz4l
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.18.0.3
    hostIPs:
    - ip: 172.18.0.3
    phase: Running
    podIP: 172.18.0.3
    podIPs:
    - ip: 172.18.0.3
    qosClass: Guaranteed
    startTime: "2024-11-04T19:34:23Z"
- metadata:
    annotations:
      kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.5:6443
      kubernetes.io/config.hash: daf3d0450842a74f77f22ba9e10a23d9
      kubernetes.io/config.mirror: daf3d0450842a74f77f22ba9e10a23d9
      kubernetes.io/config.seen: "2024-11-04T19:34:03.894831597Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-11-04T19:34:07Z"
    labels:
      component: kube-apiserver
      tier: control-plane
    name: kube-apiserver-kind-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kind-control-plane
      uid: 72c66743-5ad9-4024-a1ca-11d89edfb263
    resourceVersion: "526"
    uid: 7b95cc47-09c4-405d-8c78-b0cdd04dce1a
  spec:
    containers:
    - command:
      - kube-apiserver
      - --advertise-address=172.18.0.5
      - --allow-privileged=true
      - --authorization-mode=Node,RBAC
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --enable-admission-plugins=NodeRestriction
      - --enable-bootstrap-token-auth=true
      - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      - --etcd-servers=https://127.0.0.1:2379
      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      - --requestheader-allowed-names=front-proxy-client
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --requestheader-extra-headers-prefix=X-Remote-Extra-
      - --requestheader-group-headers=X-Remote-Group
      - --requestheader-username-headers=X-Remote-User
      - --runtime-config=
      - --secure-port=6443
      - --service-account-issuer=https://kubernetes.default.svc.cluster.local
      - --service-account-key-file=/etc/kubernetes/pki/sa.pub
      - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/16
      - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
      image: registry.k8s.io/kube-apiserver:v1.31.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 172.18.0.5
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-apiserver
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 172.18.0.5
          path: /readyz
          port: 6443
          scheme: HTTPS
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 15
      resources:
        requests:
          cpu: 250m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 172.18.0.5
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://93ee93516e7073c16c3f102defbca5af2022bb408906387e44d83fdd3317dfb6
      image: registry.k8s.io/kube-apiserver-amd64:v1.31.0
      imageID: docker.io/library/import-2024-08-13@sha256:eb4760466bf0dea0ed0a6557780dfdfb8a26bfe22302a9c023c1a45fc315ee03
      lastState: {}
      name: kube-apiserver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:05Z"
    hostIP: 172.18.0.5
    hostIPs:
    - ip: 172.18.0.5
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: Burstable
    startTime: "2024-11-04T19:34:08Z"
- metadata:
    annotations:
      kubernetes.io/config.hash: 7e9e9ec5e3483420ce6070ae7308f53f
      kubernetes.io/config.mirror: 7e9e9ec5e3483420ce6070ae7308f53f
      kubernetes.io/config.seen: "2024-11-04T19:34:08.810265527Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-11-04T19:34:08Z"
    labels:
      component: kube-controller-manager
      tier: control-plane
    name: kube-controller-manager-kind-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kind-control-plane
      uid: 72c66743-5ad9-4024-a1ca-11d89edfb263
    resourceVersion: "439"
    uid: a2ce0a5f-e719-46c6-a75d-c95d6c5b6f95
  spec:
    containers:
    - command:
      - kube-controller-manager
      - --allocate-node-cidrs=true
      - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --bind-address=127.0.0.1
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --cluster-cidr=10.244.0.0/16
      - --cluster-name=kind
      - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      - --controllers=*,bootstrapsigner,tokencleaner
      - --enable-hostpath-provisioner=true
      - --kubeconfig=/etc/kubernetes/controller-manager.conf
      - --leader-elect=true
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --root-ca-file=/etc/kubernetes/pki/ca.crt
      - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/16
      - --use-service-account-credentials=true
      image: registry.k8s.io/kube-controller-manager:v1.31.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-controller-manager
      resources:
        requests:
          cpu: 200m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        name: flexvolume-dir
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /etc/kubernetes/controller-manager.conf
        name: kubeconfig
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        type: DirectoryOrCreate
      name: flexvolume-dir
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /etc/kubernetes/controller-manager.conf
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://071ba02533120445ae753992f472a0f1ae3576fcccfd22452a359151ef829148
      image: registry.k8s.io/kube-controller-manager-amd64:v1.31.0
      imageID: docker.io/library/import-2024-08-13@sha256:881959dffed0c747196d62ab29e6d5fe1f750b709b9703db47565afa0b6acec0
      lastState: {}
      name: kube-controller-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:05Z"
    hostIP: 172.18.0.5
    hostIPs:
    - ip: 172.18.0.5
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: Burstable
    startTime: "2024-11-04T19:34:08Z"
- metadata:
    creationTimestamp: "2024-11-04T19:34:16Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 5976bc5f75
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-88txq
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: b17c7366-93c3-42e3-90b8-8b68c201469a
    resourceVersion: "411"
    uid: 19a2b5f9-9032-4f13-8f2e-93cd901480e8
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-control-plane
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.31.0
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8jrr4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-control-plane
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-8jrr4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:17Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5c611421f24676910d47a86edc41a4b5e63b03158560c669bd1402cb497c2f8f
      image: registry.k8s.io/kube-proxy-amd64:v1.31.0
      imageID: docker.io/library/import-2024-08-13@sha256:fb36548b322947288048097367669a4c44acb7bcd0475659bfd788924efdc043
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:17Z"
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8jrr4
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.18.0.5
    hostIPs:
    - ip: 172.18.0.5
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: BestEffort
    startTime: "2024-11-04T19:34:16Z"
- metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 5976bc5f75
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-hz92l
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: b17c7366-93c3-42e3-90b8-8b68c201469a
    resourceVersion: "528"
    uid: 0ad8fdf0-937b-491e-8d2e-b9f21f8e45ef
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker2
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.31.0
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qxnkh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-worker2
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-qxnkh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8e3024483af8c2a5bf9eab5d9f202bb208257837f86fad04ad02539e2cbc6fc5
      image: registry.k8s.io/kube-proxy-amd64:v1.31.0
      imageID: docker.io/library/import-2024-08-13@sha256:fb36548b322947288048097367669a4c44acb7bcd0475659bfd788924efdc043
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:23Z"
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qxnkh
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.18.0.3
    hostIPs:
    - ip: 172.18.0.3
    phase: Running
    podIP: 172.18.0.3
    podIPs:
    - ip: 172.18.0.3
    qosClass: BestEffort
    startTime: "2024-11-04T19:34:23Z"
- metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 5976bc5f75
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-k8vnb
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: b17c7366-93c3-42e3-90b8-8b68c201469a
    resourceVersion: "535"
    uid: 0920d0c3-7bbd-4bf1-abc7-0214f4ccc2c5
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker3
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.31.0
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5ks7h
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-worker3
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-5ks7h
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8fa3ed6909c67b5ae5c0a2eb8bb5bfe02db3dbf504751a493ff3b9ab254f9297
      image: registry.k8s.io/kube-proxy-amd64:v1.31.0
      imageID: docker.io/library/import-2024-08-13@sha256:fb36548b322947288048097367669a4c44acb7bcd0475659bfd788924efdc043
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:24Z"
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5ks7h
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.18.0.4
    hostIPs:
    - ip: 172.18.0.4
    phase: Running
    podIP: 172.18.0.4
    podIPs:
    - ip: 172.18.0.4
    qosClass: BestEffort
    startTime: "2024-11-04T19:34:23Z"
- metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 5976bc5f75
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-ngkmh
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: b17c7366-93c3-42e3-90b8-8b68c201469a
    resourceVersion: "536"
    uid: 28a168a8-3726-40ad-a696-3bbbe0d70603
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kind-worker
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.31.0
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8vdv4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-worker
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-8vdv4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d980f1c7122885a86d4faacf2631dce255e6207ec248cf8cb21b14590a968cae
      image: registry.k8s.io/kube-proxy-amd64:v1.31.0
      imageID: docker.io/library/import-2024-08-13@sha256:fb36548b322947288048097367669a4c44acb7bcd0475659bfd788924efdc043
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:24Z"
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8vdv4
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 172.18.0.2
    hostIPs:
    - ip: 172.18.0.2
    phase: Running
    podIP: 172.18.0.2
    podIPs:
    - ip: 172.18.0.2
    qosClass: BestEffort
    startTime: "2024-11-04T19:34:23Z"
- metadata:
    annotations:
      kubernetes.io/config.hash: 216366b07956e646a3d2a2698a971f74
      kubernetes.io/config.mirror: 216366b07956e646a3d2a2698a971f74
      kubernetes.io/config.seen: "2024-11-04T19:34:08.810266493Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2024-11-04T19:34:08Z"
    labels:
      component: kube-scheduler
      tier: control-plane
    name: kube-scheduler-kind-control-plane
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kind-control-plane
      uid: 72c66743-5ad9-4024-a1ca-11d89edfb263
    resourceVersion: "338"
    uid: 33e3ef7b-64f4-483a-9af7-a761da36d17c
  spec:
    containers:
    - command:
      - kube-scheduler
      - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      - --bind-address=127.0.0.1
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      - --leader-elect=true
      image: registry.k8s.io/kube-scheduler:v1.31.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-scheduler
      resources:
        requests:
          cpu: 100m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/scheduler.conf
        name: kubeconfig
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kind-control-plane
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/scheduler.conf
        type: FileOrCreate
      name: kubeconfig
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2024-11-04T19:34:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://253fda292a8ced0079f306827effd8c706443f58c9289748f04b45389439e947
      image: registry.k8s.io/kube-scheduler-amd64:v1.31.0
      imageID: docker.io/library/import-2024-08-13@sha256:a4448b721006ab075a7142efcd329383db5b80a19c930f7fbf58cf6e2df1eb1b
      lastState: {}
      name: kube-scheduler
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2024-11-04T19:34:05Z"
    hostIP: 172.18.0.5
    hostIPs:
    - ip: 172.18.0.5
    phase: Running
    podIP: 172.18.0.5
    podIPs:
    - ip: 172.18.0.5
    qosClass: Burstable
    startTime: "2024-11-04T19:34:08Z"
kind: PodList
metadata:
  resourceVersion: "848"
==== START logs for container coredns of pod kube-system/coredns-6f6b679f8f-c6l8s ====
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
==== END logs for container coredns of pod kube-system/coredns-6f6b679f8f-c6l8s ====
==== START logs for container coredns of pod kube-system/coredns-6f6b679f8f-zrxlc ====
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
==== END logs for container coredns of pod kube-system/coredns-6f6b679f8f-zrxlc ====
==== START logs for container etcd of pod kube-system/etcd-kind-control-plane ====
{"level":"warn","ts":"2024-11-04T19:34:05.511609Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-04T19:34:05.511732Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://172.18.0.5:2379","--cert-file=/etc/kubernetes/pki/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://172.18.0.5:2380","--initial-cluster=kind-control-plane=https://172.18.0.5:2380","--key-file=/etc/kubernetes/pki/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://172.18.0.5:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://172.18.0.5:2380","--name=kind-control-plane","--peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/etc/kubernetes/pki/etcd/peer.key","--peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt","--snapshot-count=10000","--trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt"]}
{"level":"warn","ts":"2024-11-04T19:34:05.511914Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-04T19:34:05.511947Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://172.18.0.5:2380"]}
{"level":"info","ts":"2024-11-04T19:34:05.511974Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /etc/kubernetes/pki/etcd/peer.crt, key = /etc/kubernetes/pki/etcd/peer.key, client-cert=, client-key=, trusted-ca = /etc/kubernetes/pki/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-04T19:34:05.512498Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://172.18.0.5:2379"]}
{"level":"info","ts":"2024-11-04T19:34:05.512568Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":false,"name":"kind-control-plane","data-dir":"/var/lib/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://172.18.0.5:2380"],"listen-peer-urls":["https://172.18.0.5:2380"],"advertise-client-urls":["https://172.18.0.5:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://172.18.0.5:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"kind-control-plane=https://172.18.0.5:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-11-04T19:34:05.513850Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/etcd/member/snap/db","took":"1.140061ms"}
{"level":"info","ts":"2024-11-04T19:34:05.516106Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"26e8d9c7b9b778d","cluster-id":"63d49e4051c250ad"}
{"level":"info","ts":"2024-11-04T19:34:05.516132Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"26e8d9c7b9b778d switched to configuration voters=()"}
{"level":"info","ts":"2024-11-04T19:34:05.516151Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"26e8d9c7b9b778d became follower at term 0"}
{"level":"info","ts":"2024-11-04T19:34:05.516160Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 26e8d9c7b9b778d [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-11-04T19:34:05.516164Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"26e8d9c7b9b778d became follower at term 1"}
{"level":"info","ts":"2024-11-04T19:34:05.516183Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"26e8d9c7b9b778d switched to configuration voters=(175233138742228877)"}
{"level":"warn","ts":"2024-11-04T19:34:05.518595Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-11-04T19:34:05.519660Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-11-04T19:34:05.520519Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-11-04T19:34:05.521849Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"26e8d9c7b9b778d","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-11-04T19:34:05.522040Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"26e8d9c7b9b778d","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-11-04T19:34:05.522054Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-04T19:34:05.522113Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-04T19:34:05.522125Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-04T19:34:05.522595Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-04T19:34:05.522955Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"26e8d9c7b9b778d switched to configuration voters=(175233138742228877)"}
{"level":"info","ts":"2024-11-04T19:34:05.523014Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"63d49e4051c250ad","local-member-id":"26e8d9c7b9b778d","added-peer-id":"26e8d9c7b9b778d","added-peer-peer-urls":["https://172.18.0.5:2380"]}
{"level":"info","ts":"2024-11-04T19:34:05.523630Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /etc/kubernetes/pki/etcd/server.crt, key = /etc/kubernetes/pki/etcd/server.key, client-cert=, client-key=, trusted-ca = /etc/kubernetes/pki/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-04T19:34:05.523733Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"172.18.0.5:2380"}
{"level":"info","ts":"2024-11-04T19:34:05.523750Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"172.18.0.5:2380"}
{"level":"info","ts":"2024-11-04T19:34:05.523747Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"26e8d9c7b9b778d","initial-advertise-peer-urls":["https://172.18.0.5:2380"],"listen-peer-urls":["https://172.18.0.5:2380"],"advertise-client-urls":["https://172.18.0.5:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://172.18.0.5:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-11-04T19:34:05.523764Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-11-04T19:34:06.316583Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"26e8d9c7b9b778d is starting a new election at term 1"}
{"level":"info","ts":"2024-11-04T19:34:06.316607Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"26e8d9c7b9b778d became pre-candidate at term 1"}
{"level":"info","ts":"2024-11-04T19:34:06.316626Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"26e8d9c7b9b778d received MsgPreVoteResp from 26e8d9c7b9b778d at term 1"}
{"level":"info","ts":"2024-11-04T19:34:06.316633Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"26e8d9c7b9b778d became candidate at term 2"}
{"level":"info","ts":"2024-11-04T19:34:06.316637Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"26e8d9c7b9b778d received MsgVoteResp from 26e8d9c7b9b778d at term 2"}
{"level":"info","ts":"2024-11-04T19:34:06.316642Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"26e8d9c7b9b778d became leader at term 2"}
{"level":"info","ts":"2024-11-04T19:34:06.316647Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 26e8d9c7b9b778d elected leader 26e8d9c7b9b778d at term 2"}
{"level":"info","ts":"2024-11-04T19:34:06.322067Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"26e8d9c7b9b778d","local-member-attributes":"{Name:kind-control-plane ClientURLs:[https://172.18.0.5:2379]}","request-path":"/0/members/26e8d9c7b9b778d/attributes","cluster-id":"63d49e4051c250ad","publish-timeout":"7s"}
{"level":"info","ts":"2024-11-04T19:34:06.322105Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-04T19:34:06.322149Z","caller":"etcdserver/server.go:2629","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-04T19:34:06.322145Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-04T19:34:06.322199Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-11-04T19:34:06.322210Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-11-04T19:34:06.322834Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"63d49e4051c250ad","local-member-id":"26e8d9c7b9b778d","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-04T19:34:06.322912Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-04T19:34:06.322935Z","caller":"etcdserver/server.go:2653","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-04T19:34:06.323371Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-04T19:34:06.323371Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-04T19:34:06.323867Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-11-04T19:34:06.323879Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"172.18.0.5:2379"}
==== END logs for container etcd of pod kube-system/etcd-kind-control-plane ====
==== START logs for container kindnet-cni of pod kube-system/kindnet-6f8tr ====
I1104 19:34:24.946063       1 main.go:388] probe TCP address kind-control-plane:6443
I1104 19:34:25.042992       1 main.go:109] connected to apiserver: https://kind-control-plane:6443
I1104 19:34:25.043107       1 main.go:139] hostIP = 172.18.0.4
podIP = 172.18.0.4
I1104 19:34:25.043188       1 main.go:148] setting mtu 1500 for CNI 
I1104 19:34:25.043199       1 main.go:178] kindnetd IP family: "ipv4"
I1104 19:34:25.043211       1 main.go:182] noMask IPv4 subnets: [10.244.0.0/16]
I1104 19:34:25.156213       1 controller.go:334] Starting controller kube-network-policies
I1104 19:34:25.156221       1 controller.go:338] Waiting for informer caches to sync
I1104 19:34:25.156224       1 shared_informer.go:313] Waiting for caches to sync for kube-network-policies
I1104 19:34:25.356841       1 shared_informer.go:320] Caches are synced for kube-network-policies
I1104 19:34:25.356857       1 metrics.go:61] Registering metrics
I1104 19:34:25.356880       1 controller.go:374] Syncing nftables rules
I1104 19:34:35.156905       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:35.156935       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:34:35.156999       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.0.0/24 Src: <nil> Gw: 172.18.0.5 Flags: [] Table: 0} 
I1104 19:34:35.157063       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:35.157068       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:34:35.157090       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.2.0/24 Src: <nil> Gw: 172.18.0.2 Flags: [] Table: 0} 
I1104 19:34:35.157145       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:35.157149       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:34:35.157174       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.1.0/24 Src: <nil> Gw: 172.18.0.3 Flags: [] Table: 0} 
I1104 19:34:35.157233       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:35.157237       1 main.go:299] handling current node
I1104 19:34:45.165692       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:45.165708       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:34:45.165795       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:45.165800       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:34:45.165822       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:45.165826       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:34:45.165856       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:45.165860       1 main.go:299] handling current node
I1104 19:34:55.165548       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:55.165567       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:34:55.165732       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:55.165738       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:34:55.165784       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:55.165788       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:34:55.165828       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:55.165832       1 main.go:299] handling current node
I1104 19:35:05.164615       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:05.164635       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:05.164784       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:05.164811       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:05.164865       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:05.164890       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:05.164935       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:05.164941       1 main.go:299] handling current node
I1104 19:35:15.166012       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:15.166028       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:15.166100       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:15.166104       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:15.166126       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:15.166131       1 main.go:299] handling current node
I1104 19:35:15.166138       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:15.166141       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:25.156682       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:25.156721       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:25.156848       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:25.156872       1 main.go:299] handling current node
I1104 19:35:25.156880       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:25.156883       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:25.156916       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:25.156921       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:35.160569       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:35.160589       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:35.160667       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:35.160673       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:35.160736       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:35.160741       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:35.160803       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:35.160808       1 main.go:299] handling current node
I1104 19:35:45.165579       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:45.165593       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:45.165646       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:45.165651       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:45.165674       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:45.165706       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:45.165727       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:45.165731       1 main.go:299] handling current node
I1104 19:35:55.159659       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:55.159699       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:55.159845       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:55.159857       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:55.159936       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:55.159943       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:55.159974       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:55.159980       1 main.go:299] handling current node
I1104 19:36:05.165236       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:05.165255       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:05.165343       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:05.165351       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:05.165391       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:05.165398       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:05.165428       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:05.165435       1 main.go:299] handling current node
I1104 19:36:15.160571       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:15.160597       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:15.160652       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:15.160657       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:15.160678       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:15.160682       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:15.160701       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:15.160708       1 main.go:299] handling current node
I1104 19:36:25.157060       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:25.157075       1 main.go:299] handling current node
I1104 19:36:25.157083       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:25.157086       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:25.157151       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:25.157177       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:25.157220       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:25.157224       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:35.156210       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:35.156227       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:35.156449       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:35.156485       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:35.156595       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:35.156606       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:35.156650       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:35.156666       1 main.go:299] handling current node
I1104 19:36:45.156159       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:45.156194       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:45.156272       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:45.156277       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:45.156314       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:45.156337       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:45.156358       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:45.156362       1 main.go:299] handling current node
I1104 19:36:55.164288       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:55.164304       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:55.164391       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:55.164396       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:55.164429       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:55.164436       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:55.164532       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:55.164538       1 main.go:299] handling current node
==== END logs for container kindnet-cni of pod kube-system/kindnet-6f8tr ====
==== START logs for container kindnet-cni of pod kube-system/kindnet-c2xls ====
I1104 19:34:24.885142       1 main.go:388] probe TCP address kind-control-plane:6443
I1104 19:34:24.886187       1 main.go:109] connected to apiserver: https://kind-control-plane:6443
I1104 19:34:24.977803       1 main.go:139] hostIP = 172.18.0.2
podIP = 172.18.0.2
I1104 19:34:24.977895       1 main.go:148] setting mtu 1500 for CNI 
I1104 19:34:24.977903       1 main.go:178] kindnetd IP family: "ipv4"
I1104 19:34:24.977914       1 main.go:182] noMask IPv4 subnets: [10.244.0.0/16]
I1104 19:34:25.092442       1 controller.go:334] Starting controller kube-network-policies
I1104 19:34:25.092483       1 controller.go:338] Waiting for informer caches to sync
I1104 19:34:25.092489       1 shared_informer.go:313] Waiting for caches to sync for kube-network-policies
I1104 19:34:25.293081       1 shared_informer.go:320] Caches are synced for kube-network-policies
I1104 19:34:25.293093       1 metrics.go:61] Registering metrics
I1104 19:34:25.293112       1 controller.go:374] Syncing nftables rules
I1104 19:34:35.092868       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:35.092897       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:34:35.092985       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.0.0/24 Src: <nil> Gw: 172.18.0.5 Flags: [] Table: 0} 
I1104 19:34:35.093029       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:35.093035       1 main.go:299] handling current node
I1104 19:34:35.094189       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:35.094196       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:34:35.094237       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.1.0/24 Src: <nil> Gw: 172.18.0.3 Flags: [] Table: 0} 
I1104 19:34:35.094270       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:35.094276       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:34:35.094304       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.3.0/24 Src: <nil> Gw: 172.18.0.4 Flags: [] Table: 0} 
I1104 19:34:45.101636       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:45.101656       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:34:45.101721       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:45.101726       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:34:45.101759       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:45.101784       1 main.go:299] handling current node
I1104 19:34:45.101791       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:45.101795       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:34:55.101558       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:55.101597       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:34:55.101708       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:55.101713       1 main.go:299] handling current node
I1104 19:34:55.101750       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:55.101753       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:34:55.101777       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:55.101781       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:05.102070       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:05.102091       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:05.102166       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:05.102172       1 main.go:299] handling current node
I1104 19:35:05.102180       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:05.102204       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:05.102250       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:05.102256       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:15.101525       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:15.101577       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:15.101718       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:15.101727       1 main.go:299] handling current node
I1104 19:35:15.101755       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:15.101760       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:15.101827       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:15.101832       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:25.093173       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:25.093190       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:25.093258       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:25.093264       1 main.go:299] handling current node
I1104 19:35:25.093271       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:25.093273       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:25.093322       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:25.093325       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:35.101590       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:35.101604       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:35.101664       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:35.101669       1 main.go:299] handling current node
I1104 19:35:35.101676       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:35.101679       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:35.101707       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:35.101712       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:45.101614       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:45.101632       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:45.101702       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:45.101709       1 main.go:299] handling current node
I1104 19:35:45.101718       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:45.101723       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:45.101760       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:45.101767       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:55.101672       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:55.101690       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:55.101768       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:55.101777       1 main.go:299] handling current node
I1104 19:35:55.101817       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:55.101824       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:55.101861       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:55.101880       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:05.100602       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:05.100619       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:05.100708       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:05.100714       1 main.go:299] handling current node
I1104 19:36:05.100722       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:05.100725       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:05.100752       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:05.100757       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:15.101561       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:15.101600       1 main.go:299] handling current node
I1104 19:36:15.101630       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:15.101634       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:15.101716       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:15.101722       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:15.101749       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:15.101762       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:25.093070       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:25.093086       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:25.093161       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:25.093168       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:25.093220       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:25.093225       1 main.go:299] handling current node
I1104 19:36:25.093232       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:25.093249       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:35.100367       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:35.100385       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:35.100583       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:35.100588       1 main.go:299] handling current node
I1104 19:36:35.100596       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:35.100605       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:35.100658       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:35.100662       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:45.100594       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:45.100608       1 main.go:299] handling current node
I1104 19:36:45.100616       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:45.100619       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:45.100742       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:45.100747       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:45.100803       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:45.100806       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:55.101737       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:55.101755       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:55.101827       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:55.101834       1 main.go:299] handling current node
I1104 19:36:55.101843       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:55.101876       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:55.101948       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:55.101954       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
==== END logs for container kindnet-cni of pod kube-system/kindnet-c2xls ====
==== START logs for container kindnet-cni of pod kube-system/kindnet-sl57q ====
I1104 19:34:18.389408       1 main.go:388] probe TCP address kind-control-plane:6443
I1104 19:34:18.390089       1 main.go:109] connected to apiserver: https://kind-control-plane:6443
I1104 19:34:18.390175       1 main.go:139] hostIP = 172.18.0.5
podIP = 172.18.0.5
I1104 19:34:18.390239       1 main.go:148] setting mtu 1500 for CNI 
I1104 19:34:18.390247       1 main.go:178] kindnetd IP family: "ipv4"
I1104 19:34:18.390257       1 main.go:182] noMask IPv4 subnets: [10.244.0.0/16]
I1104 19:34:18.596321       1 controller.go:334] Starting controller kube-network-policies
I1104 19:34:18.596343       1 controller.go:338] Waiting for informer caches to sync
I1104 19:34:18.596346       1 shared_informer.go:313] Waiting for caches to sync for kube-network-policies
I1104 19:34:18.731495       1 shared_informer.go:320] Caches are synced for kube-network-policies
I1104 19:34:18.731536       1 metrics.go:61] Registering metrics
I1104 19:34:18.731597       1 controller.go:374] Syncing nftables rules
I1104 19:34:18.731694       1 controller.go:359] error reading nfqueue stats: open /proc/net/netfilter/nfnetlink_queue: no such file or directory
I1104 19:34:28.598572       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:28.598595       1 main.go:299] handling current node
I1104 19:34:28.606201       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:28.606214       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:34:28.606296       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.1.0/24 Src: <nil> Gw: 172.18.0.3 Flags: [] Table: 0} 
I1104 19:34:28.606350       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:28.606356       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:34:28.606388       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.2.0/24 Src: <nil> Gw: 172.18.0.2 Flags: [] Table: 0} 
I1104 19:34:28.606420       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:28.606426       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:34:28.606455       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.3.0/24 Src: <nil> Gw: 172.18.0.4 Flags: [] Table: 0} 
I1104 19:34:38.598526       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:38.598556       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:34:38.598702       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:38.598709       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:34:38.598778       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:38.598804       1 main.go:299] handling current node
I1104 19:34:38.598812       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:38.598818       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:34:48.605622       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:48.605638       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:34:48.605705       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:48.605711       1 main.go:299] handling current node
I1104 19:34:48.605719       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:48.605724       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:34:48.605762       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:48.605770       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:34:58.596211       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:58.596227       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:34:58.596325       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:58.596330       1 main.go:299] handling current node
I1104 19:34:58.596358       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:58.596362       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:34:58.596388       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:58.596392       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:08.597578       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:08.597598       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:08.597670       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:08.597676       1 main.go:299] handling current node
I1104 19:35:08.597684       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:08.597691       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:08.597719       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:08.597724       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:18.596563       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:18.596593       1 main.go:299] handling current node
I1104 19:35:18.596603       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:18.596608       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:18.596717       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:18.596722       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:18.596769       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:18.596774       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:28.604886       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:28.604905       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:28.604982       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:28.604987       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:28.605012       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:28.605016       1 main.go:299] handling current node
I1104 19:35:28.605023       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:28.605026       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:38.605998       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:38.606046       1 main.go:299] handling current node
I1104 19:35:38.606062       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:38.606084       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:38.606217       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:38.606240       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:38.606317       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:38.606329       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:48.605505       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:48.605536       1 main.go:299] handling current node
I1104 19:35:48.605558       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:48.605562       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:48.605680       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:48.605685       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:48.605710       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:48.605714       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:58.604521       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:58.604539       1 main.go:299] handling current node
I1104 19:35:58.604550       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:58.604553       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:35:58.604639       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:58.604645       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:58.604706       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:58.604726       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:08.599543       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:08.599559       1 main.go:299] handling current node
I1104 19:36:08.599569       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:08.599573       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:08.599625       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:08.599630       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:08.599665       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:08.599669       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:18.596515       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:18.596547       1 main.go:299] handling current node
I1104 19:36:18.596558       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:18.596564       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:18.596637       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:18.596645       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:18.596673       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:18.596676       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:28.604558       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:28.604574       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:28.604678       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:28.604682       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:28.604728       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:28.604732       1 main.go:299] handling current node
I1104 19:36:28.604739       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:28.604741       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:38.596379       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:38.596398       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:38.596519       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:38.596525       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:38.596557       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:38.596562       1 main.go:299] handling current node
I1104 19:36:38.596569       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:38.596572       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
I1104 19:36:48.596362       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:48.596379       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:48.596435       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:48.596440       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:48.596523       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:48.596528       1 main.go:299] handling current node
I1104 19:36:48.596535       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:48.596537       1 main.go:322] Node kind-worker2 has CIDR [10.244.1.0/24] 
==== END logs for container kindnet-cni of pod kube-system/kindnet-sl57q ====
==== START logs for container kindnet-cni of pod kube-system/kindnet-wrm8q ====
I1104 19:34:24.704469       1 main.go:388] probe TCP address kind-control-plane:6443
I1104 19:34:24.705609       1 main.go:109] connected to apiserver: https://kind-control-plane:6443
I1104 19:34:24.705670       1 main.go:139] hostIP = 172.18.0.3
podIP = 172.18.0.3
I1104 19:34:24.705718       1 main.go:148] setting mtu 1500 for CNI 
I1104 19:34:24.705725       1 main.go:178] kindnetd IP family: "ipv4"
I1104 19:34:24.705733       1 main.go:182] noMask IPv4 subnets: [10.244.0.0/16]
I1104 19:34:24.820398       1 controller.go:334] Starting controller kube-network-policies
I1104 19:34:24.820411       1 controller.go:338] Waiting for informer caches to sync
I1104 19:34:24.820426       1 shared_informer.go:313] Waiting for caches to sync for kube-network-policies
I1104 19:34:24.969785       1 shared_informer.go:320] Caches are synced for kube-network-policies
I1104 19:34:24.969805       1 metrics.go:61] Registering metrics
I1104 19:34:24.969855       1 controller.go:374] Syncing nftables rules
I1104 19:34:34.821592       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:34.821614       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:34:34.821690       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.2.0/24 Src: <nil> Gw: 172.18.0.2 Flags: [] Table: 0} 
I1104 19:34:34.821731       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:34.821736       1 main.go:299] handling current node
I1104 19:34:34.831097       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:34.831110       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:34:34.831162       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.3.0/24 Src: <nil> Gw: 172.18.0.4 Flags: [] Table: 0} 
I1104 19:34:34.831204       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:34.831211       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:34:34.831249       1 routes.go:62] Adding route {Ifindex: 0 Dst: 10.244.0.0/24 Src: <nil> Gw: 172.18.0.5 Flags: [] Table: 0} 
I1104 19:34:44.829297       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:44.829312       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:34:44.829370       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:44.829375       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:34:44.829418       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:44.829440       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:34:44.829526       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:44.829531       1 main.go:299] handling current node
I1104 19:34:54.829440       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:34:54.829456       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:34:54.829584       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:34:54.829589       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:34:54.829614       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:34:54.829621       1 main.go:299] handling current node
I1104 19:34:54.829628       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:34:54.829631       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:04.829593       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:04.829608       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:04.829674       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:04.829679       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:04.829703       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:04.829706       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:04.829727       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:04.829730       1 main.go:299] handling current node
I1104 19:35:14.829119       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:14.829135       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:14.829209       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:14.829215       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:14.829268       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:14.829274       1 main.go:299] handling current node
I1104 19:35:14.829282       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:14.829285       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:24.820782       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:24.820798       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:24.820869       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:24.820874       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:24.820915       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:24.820920       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:24.820939       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:24.820943       1 main.go:299] handling current node
I1104 19:35:34.829104       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:34.829125       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:34.829186       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:34.829193       1 main.go:299] handling current node
I1104 19:35:34.829201       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:34.829205       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:34.829232       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:34.829236       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:44.828594       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:44.828611       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:44.828689       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:44.828694       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:44.828717       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:44.828720       1 main.go:299] handling current node
I1104 19:35:44.828727       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:44.828734       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:35:54.821030       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:35:54.821047       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:35:54.821163       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:35:54.821168       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:35:54.821193       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:35:54.821197       1 main.go:299] handling current node
I1104 19:35:54.821205       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:35:54.821208       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:04.828579       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:04.828595       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:04.828662       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:04.828668       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:04.828697       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:04.828701       1 main.go:299] handling current node
I1104 19:36:04.828709       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:04.828712       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:14.829542       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:14.829558       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:14.829621       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:14.829626       1 main.go:299] handling current node
I1104 19:36:14.829634       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:14.829636       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:14.829663       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:14.829667       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:24.820652       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:24.820680       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:24.820775       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:24.820780       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:24.820823       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:24.820828       1 main.go:299] handling current node
I1104 19:36:24.820834       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:24.820837       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:34.827516       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:34.827532       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:34.827622       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:34.827627       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:34.827677       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:34.827681       1 main.go:299] handling current node
I1104 19:36:34.827687       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:34.827691       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:44.829540       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:44.829556       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:44.829611       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:44.829615       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:44.829637       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:44.829641       1 main.go:299] handling current node
I1104 19:36:44.829648       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:44.829650       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:54.829604       1 main.go:295] Handling node with IPs: map[172.18.0.4:{}]
I1104 19:36:54.829623       1 main.go:322] Node kind-worker3 has CIDR [10.244.3.0/24] 
I1104 19:36:54.829690       1 main.go:295] Handling node with IPs: map[172.18.0.5:{}]
I1104 19:36:54.829695       1 main.go:322] Node kind-control-plane has CIDR [10.244.0.0/24] 
I1104 19:36:54.829722       1 main.go:295] Handling node with IPs: map[172.18.0.2:{}]
I1104 19:36:54.829726       1 main.go:322] Node kind-worker has CIDR [10.244.2.0/24] 
I1104 19:36:54.829751       1 main.go:295] Handling node with IPs: map[172.18.0.3:{}]
I1104 19:36:54.829756       1 main.go:299] handling current node
==== END logs for container kindnet-cni of pod kube-system/kindnet-wrm8q ====
==== START logs for container kube-apiserver of pod kube-system/kube-apiserver-kind-control-plane ====
I1104 19:34:05.172023       1 options.go:228] external host was not specified, using 172.18.0.5
I1104 19:34:05.173170       1 server.go:142] Version: v1.31.0
I1104 19:34:05.173184       1 server.go:144] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1104 19:34:05.374584       1 shared_informer.go:313] Waiting for caches to sync for node_authorizer
W1104 19:34:05.374944       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1104 19:34:05.374946       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I1104 19:34:05.378399       1 shared_informer.go:313] Waiting for caches to sync for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1104 19:34:05.380527       1 plugins.go:157] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I1104 19:34:05.380535       1 plugins.go:160] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I1104 19:34:05.380647       1 instance.go:232] Using reconciler: lease
W1104 19:34:05.381140       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I1104 19:34:06.400110       1 handler.go:286] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W1104 19:34:06.400153       1 genericapiserver.go:765] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I1104 19:34:06.491316       1 handler.go:286] Adding GroupVersion  v1 to ResourceManager
I1104 19:34:06.491451       1 apis.go:105] API group "internal.apiserver.k8s.io" is not enabled, skipping.
I1104 19:34:06.587024       1 apis.go:105] API group "storagemigration.k8s.io" is not enabled, skipping.
I1104 19:34:06.671584       1 apis.go:105] API group "resource.k8s.io" is not enabled, skipping.
I1104 19:34:06.676986       1 handler.go:286] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W1104 19:34:06.676995       1 genericapiserver.go:765] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W1104 19:34:06.676999       1 genericapiserver.go:765] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I1104 19:34:06.677264       1 handler.go:286] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W1104 19:34:06.677270       1 genericapiserver.go:765] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I1104 19:34:06.677731       1 handler.go:286] Adding GroupVersion autoscaling v2 to ResourceManager
I1104 19:34:06.678143       1 handler.go:286] Adding GroupVersion autoscaling v1 to ResourceManager
W1104 19:34:06.678149       1 genericapiserver.go:765] Skipping API autoscaling/v2beta1 because it has no resources.
W1104 19:34:06.678152       1 genericapiserver.go:765] Skipping API autoscaling/v2beta2 because it has no resources.
I1104 19:34:06.678946       1 handler.go:286] Adding GroupVersion batch v1 to ResourceManager
W1104 19:34:06.678952       1 genericapiserver.go:765] Skipping API batch/v1beta1 because it has no resources.
I1104 19:34:06.679458       1 handler.go:286] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W1104 19:34:06.679470       1 genericapiserver.go:765] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W1104 19:34:06.679476       1 genericapiserver.go:765] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I1104 19:34:06.679810       1 handler.go:286] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W1104 19:34:06.679816       1 genericapiserver.go:765] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W1104 19:34:06.679820       1 genericapiserver.go:765] Skipping API coordination.k8s.io/v1alpha1 because it has no resources.
I1104 19:34:06.680169       1 handler.go:286] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
W1104 19:34:06.680175       1 genericapiserver.go:765] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I1104 19:34:06.681110       1 handler.go:286] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W1104 19:34:06.681116       1 genericapiserver.go:765] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W1104 19:34:06.681119       1 genericapiserver.go:765] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I1104 19:34:06.681375       1 handler.go:286] Adding GroupVersion node.k8s.io v1 to ResourceManager
W1104 19:34:06.681381       1 genericapiserver.go:765] Skipping API node.k8s.io/v1beta1 because it has no resources.
W1104 19:34:06.681384       1 genericapiserver.go:765] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I1104 19:34:06.681893       1 handler.go:286] Adding GroupVersion policy v1 to ResourceManager
W1104 19:34:06.681898       1 genericapiserver.go:765] Skipping API policy/v1beta1 because it has no resources.
I1104 19:34:06.682905       1 handler.go:286] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W1104 19:34:06.682911       1 genericapiserver.go:765] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W1104 19:34:06.682914       1 genericapiserver.go:765] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I1104 19:34:06.683189       1 handler.go:286] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W1104 19:34:06.683197       1 genericapiserver.go:765] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W1104 19:34:06.683200       1 genericapiserver.go:765] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I1104 19:34:06.684447       1 handler.go:286] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W1104 19:34:06.684453       1 genericapiserver.go:765] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W1104 19:34:06.684456       1 genericapiserver.go:765] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I1104 19:34:06.685169       1 handler.go:286] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
I1104 19:34:06.685875       1 handler.go:286] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
W1104 19:34:06.685881       1 genericapiserver.go:765] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
W1104 19:34:06.685884       1 genericapiserver.go:765] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
I1104 19:34:06.688187       1 handler.go:286] Adding GroupVersion apps v1 to ResourceManager
W1104 19:34:06.688194       1 genericapiserver.go:765] Skipping API apps/v1beta2 because it has no resources.
W1104 19:34:06.688199       1 genericapiserver.go:765] Skipping API apps/v1beta1 because it has no resources.
I1104 19:34:06.689406       1 handler.go:286] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W1104 19:34:06.689412       1 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W1104 19:34:06.689416       1 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I1104 19:34:06.689765       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W1104 19:34:06.689771       1 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
I1104 19:34:06.696077       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1104 19:34:06.696093       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1104 19:34:06.895723       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1104 19:34:06.895729       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1104 19:34:06.895947       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/etc/kubernetes/pki/apiserver.crt::/etc/kubernetes/pki/apiserver.key"
I1104 19:34:06.896217       1 secure_serving.go:213] Serving securely on [::]:6443
I1104 19:34:06.896279       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1104 19:34:06.896286       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/etc/kubernetes/pki/front-proxy-client.crt::/etc/kubernetes/pki/front-proxy-client.key"
I1104 19:34:06.896302       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1104 19:34:06.896337       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1104 19:34:06.896360       1 local_available_controller.go:156] Starting LocalAvailability controller
I1104 19:34:06.896367       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1104 19:34:06.896378       1 controller.go:119] Starting legacy_token_tracking_controller
I1104 19:34:06.896383       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1104 19:34:06.896390       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1104 19:34:06.896393       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1104 19:34:06.896398       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1104 19:34:06.896405       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1104 19:34:06.896397       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1104 19:34:06.896406       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1104 19:34:06.896411       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1104 19:34:06.896542       1 aggregator.go:169] waiting for initial CRD sync...
I1104 19:34:06.896617       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1104 19:34:06.896673       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1104 19:34:06.896383       1 controller.go:78] Starting OpenAPI AggregationController
I1104 19:34:06.896711       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1104 19:34:06.896718       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1104 19:34:06.896742       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1104 19:34:06.896905       1 controller.go:142] Starting OpenAPI controller
I1104 19:34:06.896926       1 controller.go:90] Starting OpenAPI V3 controller
I1104 19:34:06.896939       1 naming_controller.go:294] Starting NamingConditionController
I1104 19:34:06.896950       1 establishing_controller.go:81] Starting EstablishingController
I1104 19:34:06.896960       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1104 19:34:06.896970       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1104 19:34:06.896988       1 crd_finalizer.go:269] Starting CRDFinalizer
I1104 19:34:06.896712       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1104 19:34:06.975669       1 shared_informer.go:320] Caches are synced for node_authorizer
I1104 19:34:06.978810       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1104 19:34:06.978817       1 policy_source.go:224] refreshing policies
I1104 19:34:06.997412       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1104 19:34:06.997466       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1104 19:34:06.997471       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1104 19:34:06.997481       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1104 19:34:06.997481       1 shared_informer.go:320] Caches are synced for configmaps
I1104 19:34:06.997492       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1104 19:34:06.997553       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1104 19:34:06.997555       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1104 19:34:06.997571       1 aggregator.go:171] initial CRD sync complete...
I1104 19:34:06.997578       1 autoregister_controller.go:144] Starting autoregister controller
I1104 19:34:06.997582       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1104 19:34:06.997587       1 cache.go:39] Caches are synced for autoregister controller
I1104 19:34:06.997673       1 cache.go:39] Caches are synced for LocalAvailability controller
I1104 19:34:06.998524       1 controller.go:615] quota admission added evaluator for: namespaces
I1104 19:34:07.007791       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1104 19:34:07.905110       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1104 19:34:07.907116       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1104 19:34:07.907124       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1104 19:34:08.121048       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1104 19:34:08.140709       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1104 19:34:08.202795       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1104 19:34:08.205666       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [172.18.0.5]
I1104 19:34:08.206233       1 controller.go:615] quota admission added evaluator for: endpoints
I1104 19:34:08.208197       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1104 19:34:09.025274       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I1104 19:34:09.029197       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1104 19:34:09.034268       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1104 19:34:09.038613       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I1104 19:34:16.404790       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1104 19:34:16.953871       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
==== END logs for container kube-apiserver of pod kube-system/kube-apiserver-kind-control-plane ====
==== START logs for container kube-controller-manager of pod kube-system/kube-controller-manager-kind-control-plane ====
I1104 19:34:05.464534       1 serving.go:386] Generated self-signed cert in-memory
I1104 19:34:05.816063       1 controllermanager.go:197] "Starting" version="v1.31.0"
I1104 19:34:05.816076       1 controllermanager.go:199] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1104 19:34:05.817112       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I1104 19:34:05.817115       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I1104 19:34:05.817245       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I1104 19:34:05.817307       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1104 19:34:05.817367       1 leaderelection.go:254] attempting to acquire leader lease kube-system/kube-controller-manager...
E1104 19:34:06.907066       1 leaderelection.go:436] error retrieving resource lock kube-system/kube-controller-manager: leases.coordination.k8s.io "kube-controller-manager" is forbidden: User "system:kube-controller-manager" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-system"
I1104 19:34:11.199974       1 leaderelection.go:268] successfully acquired lease kube-system/kube-controller-manager
I1104 19:34:11.200027       1 event.go:389] "Event occurred" object="kube-system/kube-controller-manager" fieldPath="" kind="Lease" apiVersion="coordination.k8s.io/v1" type="Normal" reason="LeaderElection" message="kind-control-plane_e59d2775-56b2-443c-b4cc-aeb10f0ca882 became leader"
I1104 19:34:11.201282       1 controllermanager.go:797] "Started controller" controller="serviceaccount-token-controller"
I1104 19:34:11.201351       1 shared_informer.go:313] Waiting for caches to sync for tokens
I1104 19:34:11.204639       1 controllermanager.go:797] "Started controller" controller="replicationcontroller-controller"
I1104 19:34:11.204741       1 replica_set.go:217] "Starting controller" logger="replicationcontroller-controller" name="replicationcontroller"
I1104 19:34:11.204749       1 shared_informer.go:313] Waiting for caches to sync for ReplicationController
I1104 19:34:11.207525       1 controllermanager.go:797] "Started controller" controller="job-controller"
I1104 19:34:11.207562       1 job_controller.go:226] "Starting job controller" logger="job-controller"
I1104 19:34:11.207570       1 shared_informer.go:313] Waiting for caches to sync for job
I1104 19:34:11.210232       1 controllermanager.go:797] "Started controller" controller="clusterrole-aggregation-controller"
I1104 19:34:11.210295       1 clusterroleaggregation_controller.go:194] "Starting ClusterRoleAggregator controller" logger="clusterrole-aggregation-controller"
I1104 19:34:11.210304       1 shared_informer.go:313] Waiting for caches to sync for ClusterRoleAggregator
E1104 19:34:11.212908       1 core.go:274] "Failed to start cloud node lifecycle controller" err="no cloud provider provided" logger="cloud-node-lifecycle-controller"
I1104 19:34:11.212923       1 controllermanager.go:775] "Warning: skipping controller" controller="cloud-node-lifecycle-controller"
I1104 19:34:11.215580       1 controllermanager.go:797] "Started controller" controller="ttl-after-finished-controller"
I1104 19:34:11.215624       1 ttlafterfinished_controller.go:112] "Starting TTL after finished controller" logger="ttl-after-finished-controller"
I1104 19:34:11.215631       1 shared_informer.go:313] Waiting for caches to sync for TTL after finished
I1104 19:34:11.219046       1 controllermanager.go:797] "Started controller" controller="deployment-controller"
I1104 19:34:11.219128       1 deployment_controller.go:173] "Starting controller" logger="deployment-controller" controller="deployment"
I1104 19:34:11.219135       1 shared_informer.go:313] Waiting for caches to sync for deployment
I1104 19:34:11.221795       1 controllermanager.go:797] "Started controller" controller="replicaset-controller"
I1104 19:34:11.221825       1 replica_set.go:217] "Starting controller" logger="replicaset-controller" name="replicaset"
I1104 19:34:11.221833       1 shared_informer.go:313] Waiting for caches to sync for ReplicaSet
I1104 19:34:11.227086       1 controllermanager.go:797] "Started controller" controller="horizontal-pod-autoscaler-controller"
I1104 19:34:11.227146       1 horizontal.go:201] "Starting HPA controller" logger="horizontal-pod-autoscaler-controller"
I1104 19:34:11.227153       1 shared_informer.go:313] Waiting for caches to sync for HPA
I1104 19:34:11.252159       1 controllermanager.go:797] "Started controller" controller="certificatesigningrequest-approving-controller"
I1104 19:34:11.252197       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-approving-controller" name="csrapproving"
I1104 19:34:11.252206       1 shared_informer.go:313] Waiting for caches to sync for certificate-csrapproving
I1104 19:34:11.302366       1 shared_informer.go:320] Caches are synced for tokens
I1104 19:34:11.407286       1 controllermanager.go:797] "Started controller" controller="ttl-controller"
I1104 19:34:11.407321       1 ttl_controller.go:127] "Starting TTL controller" logger="ttl-controller"
I1104 19:34:11.407328       1 shared_informer.go:313] Waiting for caches to sync for TTL
I1104 19:34:11.552422       1 controllermanager.go:797] "Started controller" controller="persistentvolumeclaim-protection-controller"
I1104 19:34:11.552587       1 pvc_protection_controller.go:105] "Starting PVC protection controller" logger="persistentvolumeclaim-protection-controller"
I1104 19:34:11.552599       1 shared_informer.go:313] Waiting for caches to sync for PVC protection
I1104 19:34:11.751809       1 controllermanager.go:797] "Started controller" controller="validatingadmissionpolicy-status-controller"
I1104 19:34:11.751868       1 shared_informer.go:313] Waiting for caches to sync for validatingadmissionpolicy-status
I1104 19:34:11.802328       1 controllermanager.go:797] "Started controller" controller="taint-eviction-controller"
I1104 19:34:11.802357       1 taint_eviction.go:281] "Starting" logger="taint-eviction-controller" controller="taint-eviction-controller"
I1104 19:34:11.802370       1 taint_eviction.go:287] "Sending events to api server" logger="taint-eviction-controller"
I1104 19:34:11.802382       1 shared_informer.go:313] Waiting for caches to sync for taint-eviction-controller
I1104 19:34:11.958627       1 controllermanager.go:797] "Started controller" controller="endpoints-controller"
I1104 19:34:11.958703       1 endpoints_controller.go:182] "Starting endpoint controller" logger="endpoints-controller"
I1104 19:34:11.958711       1 shared_informer.go:313] Waiting for caches to sync for endpoint
I1104 19:34:12.103110       1 controllermanager.go:797] "Started controller" controller="endpointslice-mirroring-controller"
I1104 19:34:12.103166       1 endpointslicemirroring_controller.go:227] "Starting EndpointSliceMirroring controller" logger="endpointslice-mirroring-controller"
I1104 19:34:12.103175       1 shared_informer.go:313] Waiting for caches to sync for endpoint_slice_mirroring
I1104 19:34:12.253336       1 controllermanager.go:797] "Started controller" controller="serviceaccount-controller"
I1104 19:34:12.253371       1 serviceaccounts_controller.go:114] "Starting service account controller" logger="serviceaccount-controller"
I1104 19:34:12.253377       1 shared_informer.go:313] Waiting for caches to sync for service account
I1104 19:34:12.408291       1 controllermanager.go:797] "Started controller" controller="statefulset-controller"
I1104 19:34:12.408344       1 stateful_set.go:166] "Starting stateful set controller" logger="statefulset-controller"
I1104 19:34:12.408351       1 shared_informer.go:313] Waiting for caches to sync for stateful set
I1104 19:34:12.552886       1 controllermanager.go:797] "Started controller" controller="cronjob-controller"
I1104 19:34:12.552928       1 cronjob_controllerv2.go:145] "Starting cronjob controller v2" logger="cronjob-controller"
I1104 19:34:12.552937       1 shared_informer.go:313] Waiting for caches to sync for cronjob
E1104 19:34:12.707670       1 core.go:105] "Failed to start service controller" err="WARNING: no cloud provider provided, services of type LoadBalancer will fail" logger="service-lb-controller"
I1104 19:34:12.707681       1 controllermanager.go:775] "Warning: skipping controller" controller="service-lb-controller"
I1104 19:34:12.852876       1 controllermanager.go:797] "Started controller" controller="persistentvolume-expander-controller"
I1104 19:34:12.852950       1 expand_controller.go:328] "Starting expand controller" logger="persistentvolume-expander-controller"
I1104 19:34:12.852960       1 shared_informer.go:313] Waiting for caches to sync for expand
I1104 19:34:13.007714       1 controllermanager.go:797] "Started controller" controller="root-ca-certificate-publisher-controller"
I1104 19:34:13.007726       1 controllermanager.go:749] "Controller is disabled by a feature gate" controller="storageversion-garbage-collector-controller" requiredFeatureGates=["APIServerIdentity","StorageVersionAPI"]
I1104 19:34:13.007804       1 publisher.go:107] "Starting root CA cert publisher controller" logger="root-ca-certificate-publisher-controller"
I1104 19:34:13.007815       1 shared_informer.go:313] Waiting for caches to sync for crt configmap
I1104 19:34:13.152557       1 controllermanager.go:797] "Started controller" controller="endpointslice-controller"
I1104 19:34:13.152632       1 endpointslice_controller.go:281] "Starting endpoint slice controller" logger="endpointslice-controller"
I1104 19:34:13.152641       1 shared_informer.go:313] Waiting for caches to sync for endpoint_slice
I1104 19:34:13.303629       1 controllermanager.go:797] "Started controller" controller="pod-garbage-collector-controller"
I1104 19:34:13.303663       1 gc_controller.go:99] "Starting GC controller" logger="pod-garbage-collector-controller"
I1104 19:34:13.303673       1 shared_informer.go:313] Waiting for caches to sync for GC
I1104 19:34:13.551950       1 controllermanager.go:797] "Started controller" controller="garbage-collector-controller"
I1104 19:34:13.551998       1 garbagecollector.go:146] "Starting controller" logger="garbage-collector-controller" controller="garbagecollector"
I1104 19:34:13.552009       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1104 19:34:13.552023       1 graph_builder.go:351] "Running" logger="garbage-collector-controller" component="GraphBuilder"
I1104 19:34:13.601901       1 controllermanager.go:797] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I1104 19:34:13.601915       1 controllermanager.go:775] "Warning: skipping controller" controller="storage-version-migrator-controller"
I1104 19:34:13.601930       1 cleaner.go:83] "Starting CSR cleaner controller" logger="certificatesigningrequest-cleaner-controller"
I1104 19:34:13.858430       1 controllermanager.go:797] "Started controller" controller="token-cleaner-controller"
I1104 19:34:13.858474       1 tokencleaner.go:117] "Starting token cleaner controller" logger="token-cleaner-controller"
I1104 19:34:13.858484       1 shared_informer.go:313] Waiting for caches to sync for token_cleaner
I1104 19:34:13.858490       1 shared_informer.go:320] Caches are synced for token_cleaner
I1104 19:34:14.004694       1 range_allocator.go:112] "No Secondary Service CIDR provided. Skipping filtering out secondary service addresses" logger="node-ipam-controller"
I1104 19:34:14.004714       1 controllermanager.go:797] "Started controller" controller="node-ipam-controller"
I1104 19:34:14.004833       1 node_ipam_controller.go:141] "Starting ipam controller" logger="node-ipam-controller"
I1104 19:34:14.004839       1 shared_informer.go:313] Waiting for caches to sync for node
I1104 19:34:14.052456       1 node_lifecycle_controller.go:430] "Controller will reconcile labels" logger="node-lifecycle-controller"
I1104 19:34:14.052509       1 controllermanager.go:797] "Started controller" controller="node-lifecycle-controller"
I1104 19:34:14.052513       1 controllermanager.go:749] "Controller is disabled by a feature gate" controller="resourceclaim-controller" requiredFeatureGates=["DynamicResourceAllocation"]
I1104 19:34:14.052580       1 node_lifecycle_controller.go:464] "Sending events to api server" logger="node-lifecycle-controller"
I1104 19:34:14.052589       1 node_lifecycle_controller.go:475] "Starting node controller" logger="node-lifecycle-controller"
I1104 19:34:14.052594       1 shared_informer.go:313] Waiting for caches to sync for taint
I1104 19:34:14.208380       1 controllermanager.go:797] "Started controller" controller="legacy-serviceaccount-token-cleaner-controller"
I1104 19:34:14.208417       1 legacy_serviceaccount_token_cleaner.go:103] "Starting legacy service account token cleaner controller" logger="legacy-serviceaccount-token-cleaner-controller"
I1104 19:34:14.208425       1 shared_informer.go:313] Waiting for caches to sync for legacy-service-account-token-cleaner
I1104 19:34:14.504961       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="statefulsets.apps"
I1104 19:34:14.504989       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="ingresses.networking.k8s.io"
I1104 19:34:14.505003       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="poddisruptionbudgets.policy"
I1104 19:34:14.505019       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="podtemplates"
I1104 19:34:14.505035       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="endpoints"
I1104 19:34:14.505042       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="controllerrevisions.apps"
I1104 19:34:14.505055       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="replicasets.apps"
I1104 19:34:14.505066       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="rolebindings.rbac.authorization.k8s.io"
I1104 19:34:14.505078       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="leases.coordination.k8s.io"
I1104 19:34:14.505086       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="endpointslices.discovery.k8s.io"
I1104 19:34:14.505094       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="limitranges"
I1104 19:34:14.505110       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="csistoragecapacities.storage.k8s.io"
I1104 19:34:14.505139       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="deployments.apps"
I1104 19:34:14.505150       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="networkpolicies.networking.k8s.io"
I1104 19:34:14.505182       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="serviceaccounts"
I1104 19:34:14.505193       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="daemonsets.apps"
I1104 19:34:14.505226       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="horizontalpodautoscalers.autoscaling"
I1104 19:34:14.505241       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="cronjobs.batch"
I1104 19:34:14.505256       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="jobs.batch"
I1104 19:34:14.505284       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="roles.rbac.authorization.k8s.io"
I1104 19:34:14.505302       1 controllermanager.go:797] "Started controller" controller="resourcequota-controller"
I1104 19:34:14.505335       1 resource_quota_controller.go:300] "Starting resource quota controller" logger="resourcequota-controller"
I1104 19:34:14.505346       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I1104 19:34:14.505360       1 resource_quota_monitor.go:308] "QuotaMonitor running" logger="resourcequota-controller"
I1104 19:34:14.756118       1 controllermanager.go:797] "Started controller" controller="namespace-controller"
I1104 19:34:14.756153       1 namespace_controller.go:202] "Starting namespace controller" logger="namespace-controller"
I1104 19:34:14.756159       1 shared_informer.go:313] Waiting for caches to sync for namespace
I1104 19:34:14.905000       1 controllermanager.go:797] "Started controller" controller="daemonset-controller"
I1104 19:34:14.905063       1 daemon_controller.go:294] "Starting daemon sets controller" logger="daemonset-controller"
I1104 19:34:14.905070       1 shared_informer.go:313] Waiting for caches to sync for daemon sets
I1104 19:34:14.957092       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kubelet-serving"
I1104 19:34:14.957108       1 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
I1104 19:34:14.957129       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I1104 19:34:14.957234       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kubelet-client"
I1104 19:34:14.957244       1 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-kubelet-client
I1104 19:34:14.957257       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I1104 19:34:14.957445       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kube-apiserver-client"
I1104 19:34:14.957469       1 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
I1104 19:34:14.957493       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I1104 19:34:14.957627       1 controllermanager.go:797] "Started controller" controller="certificatesigningrequest-signing-controller"
I1104 19:34:14.957635       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-legacy-unknown"
I1104 19:34:14.957641       1 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
I1104 19:34:14.957649       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
I1104 19:34:15.102546       1 controllermanager.go:797] "Started controller" controller="bootstrap-signer-controller"
I1104 19:34:15.102586       1 shared_informer.go:313] Waiting for caches to sync for bootstrap_signer
I1104 19:34:15.252440       1 controllermanager.go:797] "Started controller" controller="persistentvolume-protection-controller"
I1104 19:34:15.252477       1 pv_protection_controller.go:81] "Starting PV protection controller" logger="persistentvolume-protection-controller"
I1104 19:34:15.252483       1 shared_informer.go:313] Waiting for caches to sync for PV protection
I1104 19:34:15.407907       1 controllermanager.go:797] "Started controller" controller="ephemeral-volume-controller"
I1104 19:34:15.407917       1 controllermanager.go:749] "Controller is disabled by a feature gate" controller="service-cidr-controller" requiredFeatureGates=["MultiCIDRServiceAllocator"]
I1104 19:34:15.407946       1 controller.go:173] "Starting ephemeral volume controller" logger="ephemeral-volume-controller"
I1104 19:34:15.407953       1 shared_informer.go:313] Waiting for caches to sync for ephemeral
I1104 19:34:15.602485       1 controllermanager.go:797] "Started controller" controller="disruption-controller"
I1104 19:34:15.602512       1 core.go:298] "Warning: configure-cloud-routes is set, but no cloud provider specified. Will not configure cloud provider routes." logger="node-route-controller"
I1104 19:34:15.602522       1 controllermanager.go:775] "Warning: skipping controller" controller="node-route-controller"
I1104 19:34:15.602590       1 disruption.go:452] "Sending events to api server." logger="disruption-controller"
I1104 19:34:15.602610       1 disruption.go:463] "Starting disruption controller" logger="disruption-controller"
I1104 19:34:15.602631       1 shared_informer.go:313] Waiting for caches to sync for disruption
I1104 19:34:15.753201       1 controllermanager.go:797] "Started controller" controller="persistentvolume-binder-controller"
I1104 19:34:15.753255       1 pv_controller_base.go:308] "Starting persistent volume controller" logger="persistentvolume-binder-controller"
I1104 19:34:15.753264       1 shared_informer.go:313] Waiting for caches to sync for persistent volume
I1104 19:34:15.903249       1 controllermanager.go:797] "Started controller" controller="persistentvolume-attach-detach-controller"
I1104 19:34:15.903357       1 attach_detach_controller.go:338] "Starting attach detach controller" logger="persistentvolume-attach-detach-controller"
I1104 19:34:15.903365       1 shared_informer.go:313] Waiting for caches to sync for attach detach
I1104 19:34:15.904958       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I1104 19:34:15.906728       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"kind-control-plane\" does not exist"
I1104 19:34:15.907367       1 shared_informer.go:320] Caches are synced for TTL
I1104 19:34:15.910381       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1104 19:34:15.912257       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1104 19:34:15.916361       1 shared_informer.go:320] Caches are synced for TTL after finished
I1104 19:34:15.919559       1 shared_informer.go:320] Caches are synced for deployment
I1104 19:34:15.922789       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1104 19:34:15.927985       1 shared_informer.go:320] Caches are synced for HPA
I1104 19:34:15.952252       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I1104 19:34:15.952261       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1104 19:34:15.953402       1 shared_informer.go:320] Caches are synced for cronjob
I1104 19:34:15.953414       1 shared_informer.go:320] Caches are synced for persistent volume
I1104 19:34:15.953470       1 shared_informer.go:320] Caches are synced for PV protection
I1104 19:34:15.953482       1 shared_informer.go:320] Caches are synced for PVC protection
I1104 19:34:15.953487       1 shared_informer.go:320] Caches are synced for expand
I1104 19:34:15.953483       1 shared_informer.go:320] Caches are synced for taint
I1104 19:34:15.953489       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1104 19:34:15.953542       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1104 19:34:15.953612       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="kind-control-plane"
I1104 19:34:15.953635       1 node_lifecycle_controller.go:1036] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I1104 19:34:15.957224       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I1104 19:34:15.957305       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1104 19:34:15.958438       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1104 19:34:15.958446       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1104 19:34:15.959552       1 shared_informer.go:320] Caches are synced for endpoint
I1104 19:34:16.003021       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1104 19:34:16.003033       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1104 19:34:16.003023       1 shared_informer.go:320] Caches are synced for disruption
I1104 19:34:16.003235       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1104 19:34:16.004181       1 shared_informer.go:320] Caches are synced for GC
I1104 19:34:16.004228       1 shared_informer.go:320] Caches are synced for attach detach
I1104 19:34:16.005393       1 shared_informer.go:320] Caches are synced for node
I1104 19:34:16.005400       1 shared_informer.go:320] Caches are synced for ReplicationController
I1104 19:34:16.005448       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1104 19:34:16.005482       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1104 19:34:16.005491       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1104 19:34:16.005495       1 shared_informer.go:320] Caches are synced for cidrallocator
I1104 19:34:16.007629       1 shared_informer.go:320] Caches are synced for job
I1104 19:34:16.008747       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1104 19:34:16.008763       1 shared_informer.go:320] Caches are synced for crt configmap
I1104 19:34:16.008773       1 shared_informer.go:320] Caches are synced for ephemeral
I1104 19:34:16.008885       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="kind-control-plane" podCIDRs=["10.244.0.0/24"]
I1104 19:34:16.008901       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-control-plane"
I1104 19:34:16.008919       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-control-plane"
I1104 19:34:16.107455       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-control-plane"
I1104 19:34:16.108421       1 shared_informer.go:320] Caches are synced for stateful set
I1104 19:34:16.154068       1 shared_informer.go:320] Caches are synced for service account
I1104 19:34:16.156210       1 shared_informer.go:320] Caches are synced for namespace
I1104 19:34:16.205012       1 shared_informer.go:320] Caches are synced for resource quota
I1104 19:34:16.205525       1 shared_informer.go:320] Caches are synced for daemon sets
I1104 19:34:16.205539       1 shared_informer.go:320] Caches are synced for resource quota
I1104 19:34:16.613200       1 shared_informer.go:320] Caches are synced for garbage collector
I1104 19:34:16.652364       1 shared_informer.go:320] Caches are synced for garbage collector
I1104 19:34:16.652376       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1104 19:34:17.009831       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="598.381641ms"
I1104 19:34:17.012467       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="600.253409ms"
I1104 19:34:17.013318       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="3.464806ms"
I1104 19:34:17.013372       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="32.185µs"
I1104 19:34:17.018177       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="5.636858ms"
I1104 19:34:17.019628       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="65.714µs"
I1104 19:34:17.025581       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="26.214µs"
I1104 19:34:22.283935       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"kind-worker2\" does not exist"
I1104 19:34:22.291498       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="kind-worker2" podCIDRs=["10.244.1.0/24"]
I1104 19:34:22.291514       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker2"
I1104 19:34:22.291531       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker2"
I1104 19:34:22.526734       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"kind-worker\" does not exist"
I1104 19:34:22.533240       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="kind-worker" podCIDRs=["10.244.2.0/24"]
I1104 19:34:22.533263       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker"
I1104 19:34:22.533282       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker"
I1104 19:34:22.536872       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"kind-worker3\" does not exist"
I1104 19:34:22.541386       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="kind-worker3" podCIDRs=["10.244.3.0/24"]
I1104 19:34:22.541423       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker3"
I1104 19:34:22.541448       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker3"
I1104 19:34:22.544480       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker3"
I1104 19:34:22.680928       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker"
I1104 19:34:22.682164       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker3"
I1104 19:34:22.688241       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker2"
I1104 19:34:25.955560       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="kind-worker3"
I1104 19:34:25.955586       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="kind-worker2"
I1104 19:34:25.955605       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="kind-worker"
I1104 19:34:28.706084       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-control-plane"
I1104 19:34:28.713151       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-control-plane"
I1104 19:34:28.721756       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="53.948µs"
I1104 19:34:28.722976       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="49.796µs"
I1104 19:34:28.722976       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="32.317µs"
I1104 19:34:28.728610       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="57.029µs"
I1104 19:34:28.733273       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="43.993µs"
I1104 19:34:28.739869       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="54.216µs"
I1104 19:34:30.857552       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="41.426µs"
I1104 19:34:30.868211       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="4.79402ms"
I1104 19:34:30.868275       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="38.27µs"
I1104 19:34:30.873977       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="3.735363ms"
I1104 19:34:30.874023       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="26.243µs"
I1104 19:34:30.879067       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="2.489633ms"
I1104 19:34:30.879116       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="local-path-storage/local-path-provisioner-57c5987fd4" duration="22.538µs"
I1104 19:34:30.956371       1 node_lifecycle_controller.go:1055] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I1104 19:34:32.304813       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker2"
I1104 19:34:32.572566       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker3"
I1104 19:34:32.861788       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker"
I1104 19:34:34.900682       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="kind-worker2"
I1104 19:34:34.900710       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker2"
I1104 19:34:34.966751       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker2"
I1104 19:34:35.189078       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="kind-worker2"
I1104 19:34:35.189129       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker"
I1104 19:34:35.193517       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker"
I1104 19:34:35.196773       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="kind-worker2"
I1104 19:34:35.196782       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker3"
I1104 19:34:35.200715       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="kind-worker3"
I1104 19:34:35.956911       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
==== END logs for container kube-controller-manager of pod kube-system/kube-controller-manager-kind-control-plane ====
==== START logs for container kube-proxy of pod kube-system/kube-proxy-88txq ====
I1104 19:34:17.892682       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["172.18.0.5"]
I1104 19:34:17.892758       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_established" value=86400
I1104 19:34:17.892788       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
E1104 19:34:17.892805       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1104 19:34:17.902898       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1104 19:34:17.902931       1 server_linux.go:169] "Using iptables Proxier"
I1104 19:34:17.904217       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1104 19:34:17.904396       1 server.go:483] "Version info" version="v1.31.0"
I1104 19:34:17.904411       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1104 19:34:17.905911       1 config.go:197] "Starting service config controller"
I1104 19:34:17.905932       1 shared_informer.go:313] Waiting for caches to sync for service config
I1104 19:34:17.905948       1 config.go:326] "Starting node config controller"
I1104 19:34:17.905960       1 shared_informer.go:313] Waiting for caches to sync for node config
I1104 19:34:17.905982       1 config.go:104] "Starting endpoint slice config controller"
I1104 19:34:17.905987       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1104 19:34:18.006274       1 shared_informer.go:320] Caches are synced for service config
I1104 19:34:18.006274       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1104 19:34:18.006315       1 shared_informer.go:320] Caches are synced for node config
==== END logs for container kube-proxy of pod kube-system/kube-proxy-88txq ====
==== START logs for container kube-proxy of pod kube-system/kube-proxy-hz92l ====
I1104 19:34:24.076600       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["172.18.0.3"]
I1104 19:34:24.076726       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_established" value=86400
I1104 19:34:24.076788       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
E1104 19:34:24.076812       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1104 19:34:24.089108       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1104 19:34:24.089147       1 server_linux.go:169] "Using iptables Proxier"
I1104 19:34:24.090779       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1104 19:34:24.090989       1 server.go:483] "Version info" version="v1.31.0"
I1104 19:34:24.091006       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1104 19:34:24.093106       1 config.go:197] "Starting service config controller"
I1104 19:34:24.093144       1 shared_informer.go:313] Waiting for caches to sync for service config
I1104 19:34:24.093177       1 config.go:104] "Starting endpoint slice config controller"
I1104 19:34:24.093183       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1104 19:34:24.093251       1 config.go:326] "Starting node config controller"
I1104 19:34:24.093260       1 shared_informer.go:313] Waiting for caches to sync for node config
I1104 19:34:24.193267       1 shared_informer.go:320] Caches are synced for service config
I1104 19:34:24.193811       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1104 19:34:24.193815       1 shared_informer.go:320] Caches are synced for node config
==== END logs for container kube-proxy of pod kube-system/kube-proxy-hz92l ====
==== START logs for container kube-proxy of pod kube-system/kube-proxy-k8vnb ====
I1104 19:34:24.440658       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["172.18.0.4"]
I1104 19:34:24.440752       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_established" value=86400
I1104 19:34:24.440796       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
E1104 19:34:24.440819       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1104 19:34:24.451928       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1104 19:34:24.451967       1 server_linux.go:169] "Using iptables Proxier"
I1104 19:34:24.453830       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1104 19:34:24.454046       1 server.go:483] "Version info" version="v1.31.0"
I1104 19:34:24.454058       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1104 19:34:24.455039       1 config.go:197] "Starting service config controller"
I1104 19:34:24.455069       1 shared_informer.go:313] Waiting for caches to sync for service config
I1104 19:34:24.455087       1 config.go:326] "Starting node config controller"
I1104 19:34:24.455434       1 shared_informer.go:313] Waiting for caches to sync for node config
I1104 19:34:24.455128       1 config.go:104] "Starting endpoint slice config controller"
I1104 19:34:24.456627       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1104 19:34:24.456654       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1104 19:34:24.555971       1 shared_informer.go:320] Caches are synced for service config
I1104 19:34:24.556795       1 shared_informer.go:320] Caches are synced for node config
==== END logs for container kube-proxy of pod kube-system/kube-proxy-k8vnb ====
==== START logs for container kube-proxy of pod kube-system/kube-proxy-ngkmh ====
I1104 19:34:24.439257       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["172.18.0.2"]
I1104 19:34:24.439368       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_established" value=86400
I1104 19:34:24.439409       1 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
E1104 19:34:24.439433       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1104 19:34:24.451370       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1104 19:34:24.451416       1 server_linux.go:169] "Using iptables Proxier"
I1104 19:34:24.453037       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1104 19:34:24.453296       1 server.go:483] "Version info" version="v1.31.0"
I1104 19:34:24.453314       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1104 19:34:24.454297       1 config.go:104] "Starting endpoint slice config controller"
I1104 19:34:24.454300       1 config.go:197] "Starting service config controller"
I1104 19:34:24.454315       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1104 19:34:24.454315       1 shared_informer.go:313] Waiting for caches to sync for service config
I1104 19:34:24.454389       1 config.go:326] "Starting node config controller"
I1104 19:34:24.454409       1 shared_informer.go:313] Waiting for caches to sync for node config
I1104 19:34:24.554417       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1104 19:34:24.554417       1 shared_informer.go:320] Caches are synced for service config
I1104 19:34:24.554476       1 shared_informer.go:320] Caches are synced for node config
==== END logs for container kube-proxy of pod kube-system/kube-proxy-ngkmh ====
==== START logs for container kube-scheduler of pod kube-system/kube-scheduler-kind-control-plane ====
I1104 19:34:05.353688       1 serving.go:386] Generated self-signed cert in-memory
W1104 19:34:06.908320       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1104 19:34:06.908385       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1104 19:34:06.908472       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1104 19:34:06.908481       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1104 19:34:06.917227       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1104 19:34:06.917239       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1104 19:34:06.918624       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1104 19:34:06.918644       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1104 19:34:06.918690       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1104 19:34:06.918715       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W1104 19:34:06.919645       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1104 19:34:06.919664       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1104 19:34:06.919681       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1104 19:34:06.919681       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1104 19:34:06.919714       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1104 19:34:06.919720       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1104 19:34:06.919730       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1104 19:34:06.919739       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1104 19:34:06.919749       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E1104 19:34:06.919751       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1104 19:34:06.919764       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1104 19:34:06.919781       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1104 19:34:06.919791       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1104 19:34:06.919817       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1104 19:34:06.919815       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1104 19:34:06.919833       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1104 19:34:06.919835       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1104 19:34:06.919844       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1104 19:34:06.919844       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1104 19:34:06.919857       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1104 19:34:06.919812       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1104 19:34:06.919866       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1104 19:34:06.919868       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1104 19:34:06.919883       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1104 19:34:06.919883       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1104 19:34:06.919895       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1104 19:34:06.919899       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1104 19:34:06.919903       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1104 19:34:06.919913       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1104 19:34:06.919921       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1104 19:34:07.777614       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1104 19:34:07.777635       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1104 19:34:07.804710       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1104 19:34:07.804731       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1104 19:34:07.852991       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1104 19:34:07.853024       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1104 19:34:07.945276       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1104 19:34:07.945305       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1104 19:34:07.945666       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1104 19:34:07.945680       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1104 19:34:08.014547       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1104 19:34:08.014576       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
I1104 19:34:10.619528       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1104 19:34:11.220155       1 leaderelection.go:254] attempting to acquire leader lease kube-system/kube-scheduler...
I1104 19:34:11.222931       1 leaderelection.go:268] successfully acquired lease kube-system/kube-scheduler
==== END logs for container kube-scheduler of pod kube-system/kube-scheduler-kind-control-plane ====
---
apiVersion: v1
items:
- count: 8
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:03Z"
  involvedObject:
    kind: Node
    name: kind-control-plane
    uid: kind-control-plane
  lastTimestamp: "2024-11-04T19:34:04Z"
  message: 'Node kind-control-plane status is now: NodeHasSufficientMemory'
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kind-control-plane.1804dae753de56eb
    namespace: default
    resourceVersion: "89"
    uid: c4b40431-7aad-4bd8-bf99-3bc4ea35c3a4
  reason: NodeHasSufficientMemory
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 8
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:03Z"
  involvedObject:
    kind: Node
    name: kind-control-plane
    uid: kind-control-plane
  lastTimestamp: "2024-11-04T19:34:04Z"
  message: 'Node kind-control-plane status is now: NodeHasNoDiskPressure'
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kind-control-plane.1804dae753de6916
    namespace: default
    resourceVersion: "90"
    uid: f79469d5-5704-4fd7-9f0a-63616f6b368b
  reason: NodeHasNoDiskPressure
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 7
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:03Z"
  involvedObject:
    kind: Node
    name: kind-control-plane
    uid: kind-control-plane
  lastTimestamp: "2024-11-04T19:34:04Z"
  message: 'Node kind-control-plane status is now: NodeHasSufficientPID'
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kind-control-plane.1804dae753de72ab
    namespace: default
    resourceVersion: "88"
    uid: c3f59077-16ed-40ae-a695-4c7b184b3a6c
  reason: NodeHasSufficientPID
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:03Z"
  involvedObject:
    kind: Node
    name: kind-control-plane
    uid: kind-control-plane
  lastTimestamp: "2024-11-04T19:34:03Z"
  message: Updated Node Allocatable limit across pods
  metadata:
    creationTimestamp: "2024-11-04T19:34:07Z"
    name: kind-control-plane.1804dae75608e873
    namespace: default
    resourceVersion: "69"
    uid: 42369add-7178-4b92-92ff-9c3e34def2d9
  reason: NodeAllocatableEnforced
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:08Z"
  involvedObject:
    kind: Node
    name: kind-control-plane
    uid: kind-control-plane
  lastTimestamp: "2024-11-04T19:34:08Z"
  message: Starting kubelet.
  metadata:
    creationTimestamp: "2024-11-04T19:34:08Z"
    name: kind-control-plane.1804dae878696e89
    namespace: default
    resourceVersion: "255"
    uid: c2433ca6-ad03-465e-aa5a-8dc23b861a9f
  reason: Starting
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:08Z"
  involvedObject:
    kind: Node
    name: kind-control-plane
    uid: kind-control-plane
  lastTimestamp: "2024-11-04T19:34:08Z"
  message: Updated Node Allocatable limit across pods
  metadata:
    creationTimestamp: "2024-11-04T19:34:08Z"
    name: kind-control-plane.1804dae87a045535
    namespace: default
    resourceVersion: "258"
    uid: ed7fbbb7-95e5-42bd-b824-dd7ce440e030
  reason: NodeAllocatableEnforced
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:08Z"
  involvedObject:
    kind: Node
    name: kind-control-plane
    uid: kind-control-plane
  lastTimestamp: "2024-11-04T19:34:08Z"
  message: 'Node kind-control-plane status is now: NodeHasSufficientMemory'
  metadata:
    creationTimestamp: "2024-11-04T19:34:08Z"
    name: kind-control-plane.1804dae88003bb0e
    namespace: default
    resourceVersion: "261"
    uid: d1fb9937-f154-43a8-9f65-2498d00ffe27
  reason: NodeHasSufficientMemory
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:08Z"
  involvedObject:
    kind: Node
    name: kind-control-plane
    uid: kind-control-plane
  lastTimestamp: "2024-11-04T19:34:08Z"
  message: 'Node kind-control-plane status is now: NodeHasNoDiskPressure'
  metadata:
    creationTimestamp: "2024-11-04T19:34:08Z"
    name: kind-control-plane.1804dae88003cb15
    namespace: default
    resourceVersion: "262"
    uid: 41980783-cb6f-455f-8fac-530dc10f1dcd
  reason: NodeHasNoDiskPressure
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:08Z"
  involvedObject:
    kind: Node
    name: kind-control-plane
    uid: kind-control-plane
  lastTimestamp: "2024-11-04T19:34:08Z"
  message: 'Node kind-control-plane status is now: NodeHasSufficientPID'
  metadata:
    creationTimestamp: "2024-11-04T19:34:08Z"
    name: kind-control-plane.1804dae88003d1c9
    namespace: default
    resourceVersion: "263"
    uid: c2ae0b63-fd17-499b-8145-402a998d008d
  reason: NodeHasSufficientPID
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:15Z"
  involvedObject:
    apiVersion: v1
    kind: Node
    name: kind-control-plane
    uid: 72c66743-5ad9-4024-a1ca-11d89edfb263
  lastTimestamp: "2024-11-04T19:34:15Z"
  message: 'Node kind-control-plane event: Registered Node kind-control-plane in Controller'
  metadata:
    creationTimestamp: "2024-11-04T19:34:15Z"
    name: kind-control-plane.1804daea2286155c
    namespace: default
    resourceVersion: "340"
    uid: db6fda9c-edf2-47ad-aa83-29c52e7ea990
  reason: RegisteredNode
  reportingComponent: node-controller
  reportingInstance: ""
  source:
    component: node-controller
  type: Normal
- action: StartKubeProxy
  eventTime: "2024-11-04T19:34:17.905926Z"
  firstTimestamp: null
  involvedObject:
    kind: Node
    name: kind-control-plane
    uid: kind-control-plane
  lastTimestamp: null
  metadata:
    creationTimestamp: "2024-11-04T19:34:17Z"
    name: kind-control-plane.1804daea96e5525f
    namespace: default
    resourceVersion: "413"
    uid: 93b4fd21-7778-4b74-bd6e-af5fbe6b7409
  reason: Starting
  reportingComponent: kube-proxy
  reportingInstance: kube-proxy-kind-control-plane
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:28Z"
  involvedObject:
    kind: Node
    name: kind-control-plane
    uid: kind-control-plane
  lastTimestamp: "2024-11-04T19:34:28Z"
  message: 'Node kind-control-plane status is now: NodeReady'
  metadata:
    creationTimestamp: "2024-11-04T19:34:28Z"
    name: kind-control-plane.1804daed19edefb2
    namespace: default
    resourceVersion: "561"
    uid: f75ac210-81d7-49bd-af30-234c85c648a3
  reason: NodeReady
  reportingComponent: kubelet
  reportingInstance: kind-control-plane
  source:
    component: kubelet
    host: kind-control-plane
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:21Z"
  involvedObject:
    kind: Node
    name: kind-worker
    uid: kind-worker
  lastTimestamp: "2024-11-04T19:34:21Z"
  message: 'Node kind-worker status is now: NodeHasNoDiskPressure'
  metadata:
    creationTimestamp: "2024-11-04T19:34:21Z"
    name: kind-worker.1804daeb5bc87e6f
    namespace: default
    resourceVersion: "434"
    uid: 969a7dff-e227-4cf8-af2d-b39f8f2ee8f2
  reason: NodeHasNoDiskPressure
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:21Z"
  involvedObject:
    kind: Node
    name: kind-worker
    uid: kind-worker
  lastTimestamp: "2024-11-04T19:34:21Z"
  message: 'Node kind-worker status is now: NodeHasSufficientPID'
  metadata:
    creationTimestamp: "2024-11-04T19:34:21Z"
    name: kind-worker.1804daeb5bc88c00
    namespace: default
    resourceVersion: "436"
    uid: a7caac3f-475a-4911-97c3-8fff412277fd
  reason: NodeHasSufficientPID
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker
    uid: kind-worker
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Starting kubelet.
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker.1804daeba2757395
    namespace: default
    resourceVersion: "461"
    uid: 7d6c56c1-cd7c-4e61-adc1-daa88c009cb3
  reason: Starting
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker
    uid: kind-worker
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Node kind-worker status is now: NodeHasSufficientMemory'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker.1804daeba3244b6b
    namespace: default
    resourceVersion: "473"
    uid: e81f9b3b-3104-4742-9f92-aa8a18db3a3c
  reason: NodeHasSufficientMemory
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker
    uid: kind-worker
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Node kind-worker status is now: NodeHasNoDiskPressure'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker.1804daeba3246081
    namespace: default
    resourceVersion: "477"
    uid: 4862a829-7018-4b2a-b447-5015d017a23b
  reason: NodeHasNoDiskPressure
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker
    uid: kind-worker
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Node kind-worker status is now: NodeHasSufficientPID'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker.1804daeba3246a9b
    namespace: default
    resourceVersion: "484"
    uid: e54fb8f5-526e-4e95-9196-eb0aa56191aa
  reason: NodeHasSufficientPID
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker
    uid: kind-worker
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Updated Node Allocatable limit across pods
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker.1804daeba428974d
    namespace: default
    resourceVersion: "469"
    uid: 8801a89d-bd37-4dfd-ab23-6e284e212143
  reason: NodeAllocatableEnforced
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- action: StartKubeProxy
  eventTime: "2024-11-04T19:34:24.454280Z"
  firstTimestamp: null
  involvedObject:
    kind: Node
    name: kind-worker
    uid: kind-worker
  lastTimestamp: null
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kind-worker.1804daec1d354598
    namespace: default
    resourceVersion: "539"
    uid: a22ac07e-62e2-499c-ad6f-755a1a389f3b
  reason: Starting
  reportingComponent: kube-proxy
  reportingInstance: kube-proxy-kind-worker
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:25Z"
  involvedObject:
    apiVersion: v1
    kind: Node
    name: kind-worker
    uid: e645302f-6781-4867-9a73-70228c9929b0
  lastTimestamp: "2024-11-04T19:34:25Z"
  message: 'Node kind-worker event: Registered Node kind-worker in Controller'
  metadata:
    creationTimestamp: "2024-11-04T19:34:25Z"
    name: kind-worker.1804daec76b07e7b
    namespace: default
    resourceVersion: "556"
    uid: b10a9037-b5c9-40a7-98b1-4846a4eaf719
  reason: RegisteredNode
  reportingComponent: node-controller
  reportingInstance: ""
  source:
    component: node-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:35Z"
  involvedObject:
    kind: Node
    name: kind-worker
    uid: kind-worker
  lastTimestamp: "2024-11-04T19:34:35Z"
  message: 'Node kind-worker status is now: NodeReady'
  metadata:
    creationTimestamp: "2024-11-04T19:34:35Z"
    name: kind-worker.1804daee9cd58fe0
    namespace: default
    resourceVersion: "614"
    uid: 556db484-04d8-4c94-9074-0aa27325e4b2
  reason: NodeReady
  reportingComponent: kubelet
  reportingInstance: kind-worker
  source:
    component: kubelet
    host: kind-worker
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker2
    uid: kind-worker2
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Starting kubelet.
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker2.1804daeb93a091e5
    namespace: default
    resourceVersion: "440"
    uid: 6089f26c-ad07-4745-bd15-633b6ac861e9
  reason: Starting
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker2
    uid: kind-worker2
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Node kind-worker2 status is now: NodeHasSufficientMemory'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker2.1804daeb94128e29
    namespace: default
    resourceVersion: "446"
    uid: eb675dcb-fd2e-4503-bbfa-3a53f552b533
  reason: NodeHasSufficientMemory
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker2
    uid: kind-worker2
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Node kind-worker2 status is now: NodeHasNoDiskPressure'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker2.1804daeb9412a0ea
    namespace: default
    resourceVersion: "450"
    uid: f3896330-af20-4b22-959c-f1633e3d5de6
  reason: NodeHasNoDiskPressure
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker2
    uid: kind-worker2
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Node kind-worker2 status is now: NodeHasSufficientPID'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker2.1804daeb9412a6f6
    namespace: default
    resourceVersion: "455"
    uid: 60f96dd4-279a-444b-82db-76dcac364d6a
  reason: NodeHasSufficientPID
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker2
    uid: kind-worker2
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Updated Node Allocatable limit across pods
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker2.1804daeb95ad443a
    namespace: default
    resourceVersion: "444"
    uid: 13ad8809-7fff-4b59-9fdb-17aaca6aaa1f
  reason: NodeAllocatableEnforced
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- action: StartKubeProxy
  eventTime: "2024-11-04T19:34:24.093051Z"
  firstTimestamp: null
  involvedObject:
    kind: Node
    name: kind-worker2
    uid: kind-worker2
  lastTimestamp: null
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kind-worker2.1804daec07ad5d90
    namespace: default
    resourceVersion: "527"
    uid: f2dce84e-7a24-4649-809f-dc4927bb7405
  reason: Starting
  reportingComponent: kube-proxy
  reportingInstance: kube-proxy-kind-worker2
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:25Z"
  involvedObject:
    apiVersion: v1
    kind: Node
    name: kind-worker2
    uid: f9cbe64a-206c-453a-b165-d02cacca1105
  lastTimestamp: "2024-11-04T19:34:25Z"
  message: 'Node kind-worker2 event: Registered Node kind-worker2 in Controller'
  metadata:
    creationTimestamp: "2024-11-04T19:34:25Z"
    name: kind-worker2.1804daec76b077eb
    namespace: default
    resourceVersion: "555"
    uid: 8cc86a5e-b85a-454b-ba1e-01698f9caa17
  reason: RegisteredNode
  reportingComponent: node-controller
  reportingInstance: ""
  source:
    component: node-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:34Z"
  involvedObject:
    kind: Node
    name: kind-worker2
    uid: kind-worker2
  lastTimestamp: "2024-11-04T19:34:34Z"
  message: 'Node kind-worker2 status is now: NodeReady'
  metadata:
    creationTimestamp: "2024-11-04T19:34:34Z"
    name: kind-worker2.1804daee88b9220c
    namespace: default
    resourceVersion: "611"
    uid: b3b0cdb3-cf6e-4694-b747-05ecfeef45ef
  reason: NodeReady
  reportingComponent: kubelet
  reportingInstance: kind-worker2
  source:
    component: kubelet
    host: kind-worker2
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:21Z"
  involvedObject:
    kind: Node
    name: kind-worker3
    uid: kind-worker3
  lastTimestamp: "2024-11-04T19:34:21Z"
  message: 'Node kind-worker3 status is now: NodeHasSufficientMemory'
  metadata:
    creationTimestamp: "2024-11-04T19:34:21Z"
    name: kind-worker3.1804daeb51b94274
    namespace: default
    resourceVersion: "428"
    uid: 30999c19-b720-4d66-b8ec-ef3a60b65813
  reason: NodeHasSufficientMemory
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:21Z"
  involvedObject:
    kind: Node
    name: kind-worker3
    uid: kind-worker3
  lastTimestamp: "2024-11-04T19:34:21Z"
  message: 'Node kind-worker3 status is now: NodeHasNoDiskPressure'
  metadata:
    creationTimestamp: "2024-11-04T19:34:21Z"
    name: kind-worker3.1804daeb51b94ee6
    namespace: default
    resourceVersion: "429"
    uid: 4e66f252-52b3-4e11-9bab-faaddbefcb1c
  reason: NodeHasNoDiskPressure
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:21Z"
  involvedObject:
    kind: Node
    name: kind-worker3
    uid: kind-worker3
  lastTimestamp: "2024-11-04T19:34:21Z"
  message: 'Node kind-worker3 status is now: NodeHasSufficientPID'
  metadata:
    creationTimestamp: "2024-11-04T19:34:21Z"
    name: kind-worker3.1804daeb51b957ba
    namespace: default
    resourceVersion: "430"
    uid: 3b1ec5c1-c5a8-49eb-a9ab-d6e81627d0ca
  reason: NodeHasSufficientPID
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker3
    uid: kind-worker3
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Starting kubelet.
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker3.1804daeba298ef00
    namespace: default
    resourceVersion: "462"
    uid: 5a5cf36a-18e5-431c-a5c2-954dbe62c3ea
  reason: Starting
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker3
    uid: kind-worker3
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Node kind-worker3 status is now: NodeHasSufficientMemory'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker3.1804daeba3626527
    namespace: default
    resourceVersion: "488"
    uid: 52cf3e66-1dbe-4fc2-8dc6-ea3ce3acf70d
  reason: NodeHasSufficientMemory
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker3
    uid: kind-worker3
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Node kind-worker3 status is now: NodeHasNoDiskPressure'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker3.1804daeba36278a6
    namespace: default
    resourceVersion: "496"
    uid: d0a4f952-c1a4-4ae3-bb70-2bb8c1fd4af9
  reason: NodeHasNoDiskPressure
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 2
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker3
    uid: kind-worker3
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: 'Node kind-worker3 status is now: NodeHasSufficientPID'
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker3.1804daeba3628390
    namespace: default
    resourceVersion: "502"
    uid: 78fc3e44-4e81-4776-9f35-839121009b2e
  reason: NodeHasSufficientPID
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:22Z"
  involvedObject:
    kind: Node
    name: kind-worker3
    uid: kind-worker3
  lastTimestamp: "2024-11-04T19:34:22Z"
  message: Updated Node Allocatable limit across pods
  metadata:
    creationTimestamp: "2024-11-04T19:34:22Z"
    name: kind-worker3.1804daeba4b1f7c3
    namespace: default
    resourceVersion: "470"
    uid: 43cfd31e-3a82-4dc4-be02-b6f530a90637
  reason: NodeAllocatableEnforced
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
- action: StartKubeProxy
  eventTime: "2024-11-04T19:34:24.454976Z"
  firstTimestamp: null
  involvedObject:
    kind: Node
    name: kind-worker3
    uid: kind-worker3
  lastTimestamp: null
  metadata:
    creationTimestamp: "2024-11-04T19:34:24Z"
    name: kind-worker3.1804daec1d3fe635
    namespace: default
    resourceVersion: "540"
    uid: 143e7369-7bdb-4f6e-91ab-27318caea058
  reason: Starting
  reportingComponent: kube-proxy
  reportingInstance: kube-proxy-kind-worker3
  source: {}
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:25Z"
  involvedObject:
    apiVersion: v1
    kind: Node
    name: kind-worker3
    uid: eb42267d-a6f3-400a-bd3b-7eecc9b01bcd
  lastTimestamp: "2024-11-04T19:34:25Z"
  message: 'Node kind-worker3 event: Registered Node kind-worker3 in Controller'
  metadata:
    creationTimestamp: "2024-11-04T19:34:25Z"
    name: kind-worker3.1804daec76b0640f
    namespace: default
    resourceVersion: "554"
    uid: 89858d7f-1dce-465e-a34d-ab95e019ec89
  reason: RegisteredNode
  reportingComponent: node-controller
  reportingInstance: ""
  source:
    component: node-controller
  type: Normal
- count: 1
  eventTime: null
  firstTimestamp: "2024-11-04T19:34:35Z"
  involvedObject:
    kind: Node
    name: kind-worker3
    uid: kind-worker3
  lastTimestamp: "2024-11-04T19:34:35Z"
  message: 'Node kind-worker3 status is now: NodeReady'
  metadata:
    creationTimestamp: "2024-11-04T19:34:35Z"
    name: kind-worker3.1804daee9d48c6e3
    namespace: default
    resourceVersion: "617"
    uid: 94cffb62-d984-4c9c-9953-09a0ea842759
  reason: NodeReady
  reportingComponent: kubelet
  reportingInstance: kind-worker3
  source:
    component: kubelet
    host: kind-worker3
  type: Normal
kind: EventList
metadata:
  resourceVersion: "848"
---
apiVersion: v1
items: []
kind: ReplicationControllerList
metadata:
  resourceVersion: "848"
---
apiVersion: v1
items:
- metadata:
    creationTimestamp: "2024-11-04T19:34:08Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "232"
    uid: a46b75a6-3410-442d-865c-a528fb5f94fe
  spec:
    clusterIP: 10.96.0.1
    clusterIPs:
    - 10.96.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
kind: ServiceList
metadata:
  resourceVersion: "848"
---
apiVersion: apps/v1
items: []
kind: DaemonSetList
metadata:
  resourceVersion: "848"
---
apiVersion: apps/v1
items: []
kind: DeploymentList
metadata:
  resourceVersion: "848"
---
apiVersion: apps/v1
items: []
kind: ReplicaSetList
metadata:
  resourceVersion: "848"
---
apiVersion: v1
items: []
kind: PodList
metadata:
  resourceVersion: "848"
